{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from functools import reduce\n",
    "import pprint\n",
    "from prettytable import PrettyTable\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "from phrasefinder import phrasefinder as pf\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "from functools import reduce\n",
    "from tinydb import TinyDB, Query\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient\n",
    "from google.cloud.bigquery_storage import types\n",
    "from google.cloud import bigquery\n",
    "comprehend = boto3.client(service_name='comprehend', region_name='us-east-1')\n",
    "\n",
    "\n",
    "bigquery_client = None\n",
    "bigquery_data = None\n",
    "\n",
    "def getBestVideoList(keywords, topn=10):\n",
    "    scopes = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"0\"\n",
    "\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    client_secrets_file = \"./google_credentials/client_secret_754636752811-rmth1g8e3dl144jda8fddh1ihhj413um.apps.googleusercontent.com.json\"\n",
    "\n",
    "    # Get credentials and create an API client\n",
    "    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n",
    "        client_secrets_file, scopes)\n",
    "\n",
    "    credentials = Credentials(\n",
    "        None,\n",
    "        refresh_token=\"1//0fNppFYz3o7ABCgYIARAAGA8SNwF-L9IrgIZJAKCn9iSH_172SxyT6cA3mMHDlSQ0MTj9MmKTc6zZRnSy1nwMW5kRkl52JYb4jhg\",\n",
    "        token_uri=\"https://accounts.google.com/o/oauth2/token\",\n",
    "        client_id=\"754636752811-rmth1g8e3dl144jda8fddh1ihhj413um.apps.googleusercontent.com\",\n",
    "        client_secret=\"KhUufHmhS8XI0srgpP__cTCr\")\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, credentials=credentials)\n",
    "\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        maxResults=topn,\n",
    "        q=keywords,\n",
    "        relevanceLanguage='en'\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items']\n",
    "\n",
    "\n",
    "\"\"\"     best_content_list = []\n",
    "    with os.scandir('./data') as entries:\n",
    "        for entry in entries:\n",
    "            with open('./data/' + entry.name, 'r') as file:\n",
    "                best_content_list.append(file.read())\n",
    "    return best_content_list \"\"\"\n",
    "\n",
    "\n",
    "def getKeyPhrases(content_list):\n",
    "    def insertFrequency(keyPhrase):\n",
    "        frequency = {'overall': 0, 'min': 0,\n",
    "                     'max': 0, 'frequency_by_content': []}\n",
    "        for content in content_list:\n",
    "            _freq = content.count(keyPhrase['Text'])\n",
    "            frequency['overall'] += _freq\n",
    "            frequency['min'] = _freq if frequency['min'] == 0 else min(\n",
    "                frequency['min'], _freq)\n",
    "            frequency['max'] = max(frequency['max'], _freq)\n",
    "            frequency['frequency_by_content'].append(_freq)\n",
    "\n",
    "        frequency['doc_freq'] = 0\n",
    "        for e in frequency['frequency_by_content']:\n",
    "            frequency['doc_freq'] += e > 0\n",
    "\n",
    "        frequency['median'] = statistics.median(\n",
    "            frequency['frequency_by_content'])\n",
    "        return {**keyPhrase, 'frequency': frequency}\n",
    "\n",
    "\n",
    "    def getNgramDataFromPhraseFinder(phrase):\n",
    "        ngramData = {'text': phrase}\n",
    "\n",
    "        # Optional: set the maximum number of phrases to return.\n",
    "        options = pf.SearchOptions()\n",
    "        options.topk = 1\n",
    "\n",
    "        # Send the request.\n",
    "        try:\n",
    "            result = pf.search(pf.Corpus.AMERICAN_ENGLISH, phrase, options)\n",
    "            if result.error:\n",
    "                print('Request for {} was not successful: {}'.format(\n",
    "                    phrase, result.error['message']))\n",
    "                return\n",
    "\n",
    "            time.sleep(0.02)\n",
    "            ngramData = {\n",
    "                **ngramData, 'volume_count': result.phrases[0].volume_count,\n",
    "                'match_count': result.phrases[0].match_count}\n",
    "        except Exception as error:\n",
    "            print('Fatal error for {}: {}'.format(phrase, error))\n",
    "            ngramData = {**ngramData, 'volume_count': 0, 'match_count': 0}\n",
    "        \n",
    "        return ngramData\n",
    "\n",
    "    def setupBiqqueryClient():\n",
    "        global bigquery_data\n",
    "        bigquery_client = bigquery.Client()\n",
    "        table_id = 'ngram_reddit_2019_08.ngram_reddit_2019_08'\n",
    "        bigquery_data = {}\n",
    "      \n",
    "        with open('./ngram.json', 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                data = json.loads(line)\n",
    "                bigquery_data[data['phrase']] = {'phrase': data['phrase'], 'volume_count': int(data['volume_count']), 'match_count': int(data['match_count'])}\n",
    "\n",
    "    def getNgramDataFromBigQuery(phrase):\n",
    "        global bigquery_data\n",
    "\n",
    "        ngramData = {'text': phrase}\n",
    "\n",
    "        if bigquery_data is None:\n",
    "            setupBiqqueryClient()\n",
    "        '''\n",
    "        client = bigquery.Client()\n",
    "        query_job = client.query(\n",
    "            \"\"\"\n",
    "            SELECT *\n",
    "            FROM `wallet-1546746037952.ngram_reddit_2019_08.ngram_reddit_2019_08`\n",
    "            WHERE phrase = \"{0}\"\n",
    "            \"\"\".format(phrase)\n",
    "        )\n",
    "        results = query_job.result()\n",
    "        \n",
    "        for row in results:\n",
    "            ngramData = {**ngramData, 'volume_count': row.volume_count, 'match_count': row.match_count}\n",
    "            return ngramData\n",
    "        '''\n",
    "\n",
    "        _str = phrase.lower()\n",
    "        if _str not in bigquery_data:\n",
    "            print('Fatal error for {}'.format(_str))\n",
    "            ngramData = {**ngramData, 'volume_count': 0, 'match_count': 0}\n",
    "        else:\n",
    "            ngramData = bigquery_data[_str]\n",
    "        return ngramData\n",
    "        \n",
    "\n",
    "    def insertNgramFrequency(keyPhrase):\n",
    "\n",
    "        db = TinyDB('db.json')\n",
    "        ngramQuery = Query()\n",
    "\n",
    "        # Set up your query.\n",
    "        query = keyPhrase['Text']\n",
    "\n",
    "        ngramData = db.get(ngramQuery.text == query)\n",
    "\n",
    "        if ngramData is None:\n",
    "            # ngramData = getNgramDataFromPhraseFinder(phrase)\n",
    "            ngramData = getNgramDataFromBigQuery(query)\n",
    "            # cache ngramData\n",
    "            db.insert(ngramData)\n",
    "\n",
    "        return {**keyPhrase, 'ngram_volume_count': ngramData['volume_count'], 'ngram_match_count': ngramData['match_count']}\n",
    "\n",
    "    def filterPhrases(phrasesByContent):\n",
    "        phrases = reduce(lambda x, y: x+y, phrasesByContent)\n",
    "        # trim stop words from begining and end\n",
    "\n",
    "        def stripStopWords(phrase):\n",
    "            _stop_words = [\n",
    "                \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "                \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "                \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n",
    "                \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n",
    "                \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\",\n",
    "                \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\",\n",
    "                \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"in\", \"out\", \"on\", \"off\",\n",
    "                \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
    "                \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\",\n",
    "                \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\",\n",
    "                \"now\"] + [\"yeah\", \"thing\", \"things\", \"hey\", \"ohh\", \"really\"]\n",
    "\n",
    "            while True:\n",
    "                # repetitively trim stop words from the beginning and the end\n",
    "                modified = False\n",
    "                _str = phrase['Text'].lower()\n",
    "                for w in _stop_words:\n",
    "                    if _str.startswith(w+' '):\n",
    "                        phrase['Text'] = phrase['Text'][len(w)+1:]\n",
    "                        modified = True\n",
    "                        break\n",
    "                    elif _str.endswith(' '+w):\n",
    "                        phrase['Text'] = phrase['Text'][:-len(w)-1]\n",
    "                        modified = True\n",
    "                        break\n",
    "                    elif _str == w:\n",
    "                        phrase['Text'] = \"\"\n",
    "                        break\n",
    "                if not modified:\n",
    "                    break\n",
    "            return phrase\n",
    "\n",
    "        phrases = list(map(lambda x: {**x, 'ori_text': x['Text']}, phrases))\n",
    "\n",
    "        phrases = list(map(stripStopWords, phrases))\n",
    "\n",
    "        # only keep bigrams\n",
    "        phrases = list(filter(lambda x: len(\n",
    "            x['Text'].split(' ')) >= 1, phrases))\n",
    "\n",
    "        # remove duplicates\n",
    "        unique_phrases = []\n",
    "        phrase_set = set()\n",
    "        for phrase in phrases:\n",
    "            if phrase['Text'] not in phrase_set:\n",
    "                phrase_set.add(phrase['Text'])\n",
    "                unique_phrases.append(phrase)\n",
    "\n",
    "        print('# Unique phrases: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # min keyword score\n",
    "        unique_phrases = list(\n",
    "            filter(lambda x: x['Score'] >= 0.999, unique_phrases))\n",
    "\n",
    "        # add frequency for each phrase\n",
    "        unique_phrases = list(map(insertFrequency, unique_phrases))\n",
    "\n",
    "        # keep phrases showing up at least once in all documents\n",
    "        unique_phrases = list(\n",
    "            filter(lambda x: x['frequency']['min'] >= 0, unique_phrases))\n",
    "\n",
    "        # phrase should have at least 3 chars\n",
    "        unique_phrases = list(\n",
    "            filter(lambda x: len(x['Text']) >= 4, unique_phrases))\n",
    "\n",
    "        # phrase should not have weird symbols\n",
    "        unique_phrases = list(filter(lambda phrase: all(x.isalpha() or x.isspace(\n",
    "        ) or (x == '-' and idx > 0 and idx < len(phrase['Text'])-1) or x.isdigit() for idx, x in enumerate(phrase['Text'])), unique_phrases))\n",
    "\n",
    "        print('# phrases: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # add ngrams frequency for each phrase\n",
    "        for i in tqdm(range(len(unique_phrases))):\n",
    "            unique_phrases[i] = insertNgramFrequency(unique_phrases[i])\n",
    "        #unique_phrases = list(map(insertNgramFrequency, unique_phrases))\n",
    "\n",
    "        phrases = unique_phrases\n",
    "\n",
    "        # sort key phrases by overall frequency\n",
    "        phrases.sort(reverse=True, key=lambda x: x['frequency']['overall'])\n",
    "\n",
    "        print(len(phrases))\n",
    "\n",
    "        # return most frequent top phrases\n",
    "        return phrases\n",
    "\n",
    "    # comprehend api requires each content to be less than 5000 bytes\n",
    "    # see https://docs.aws.amazon.com/comprehend/latest/dg/guidelines-and-limits.html\n",
    "    content_list_splited = []\n",
    "    for content in content_list:\n",
    "        content_list_splited.extend([content[i: i+4000]\n",
    "                                     for i in range(0, len(content), 4000)])\n",
    "    \"\"\" for content in content_list_splited:\n",
    "        print(len(content))\n",
    " \"\"\"\n",
    "    def chunks(l, n):\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "\n",
    "    content_list_splited_chunks = list(chunks(content_list_splited, 25))\n",
    "\n",
    "    keyPhrasesByContent = []\n",
    "    entitiesByContent = []\n",
    "    for chunk in content_list_splited_chunks:\n",
    "        keyPhrasesResponse = comprehend.batch_detect_key_phrases(\n",
    "            TextList=chunk, LanguageCode='en')\n",
    "        entitiesResponse = comprehend.batch_detect_entities(\n",
    "            TextList=chunk, LanguageCode='en')\n",
    "\n",
    "        keyPhrasesByContent.extend(\n",
    "            list(map(lambda x: x['KeyPhrases'], keyPhrasesResponse['ResultList'])))\n",
    "        entitiesByContent.extend(\n",
    "            list(map(lambda x: x['Entities'], entitiesResponse['ResultList'])))\n",
    "\n",
    "    # return filterPhrases(keyPhrasesByContent), filterPhrases(entitiesByContent)\n",
    "    return filterPhrases(keyPhrasesByContent)\n",
    "\n",
    "\n",
    "def getTranscript(video_id):\n",
    "    print('Getting transcript for '+video_id)\n",
    "    return YouTubeTranscriptApi.get_transcript(video_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Working on keywords: how do i get my annual free credit report\n",
      "Getting transcript for hjt3KOVl8UE\n",
      "Getting transcript for yah0ObWdjZ8\n",
      "Getting transcript for 1qnNwVAFNWc\n",
      "Getting transcript for hbkdl5bDL-Q\n",
      "Getting transcript for 7avKKr9BLeU\n",
      "Getting transcript for 12D4NqcfU7s\n",
      "Getting transcript for WOnTlYzbHhk\n",
      "Getting transcript for R0iwXPAol_U\n",
      "Getting transcript for yXJrE_wW_pM\n",
      "Getting transcript for 5l_IpNe1koU\n",
      "Getting transcript for 1Ybm1VomEeM\n",
      "Getting transcript for hpJGK_ytnDI\n",
      "Getting transcript for 51EkxpbyVqM\n",
      "Getting transcript for -wd-06QPwDw\n",
      "Getting transcript for OOYY11DAGRA\n",
      "Getting transcript for ZPvRYAnZcx4\n",
      "Getting transcript for XNBnOssQPXo\n",
      "Getting transcript for EUd0Z4rkuPY\n",
      "Getting transcript for rKMUUZ-92xg\n",
      "Getting transcript for d_6WpFJtFFA\n",
      "Getting transcript for gJNw1W9VmQc\n",
      "Getting transcript for PcvU_S0F8Y4\n",
      "Getting transcript for CKIu9sZoaQo\n",
      "Getting transcript for LJApqsKunps\n",
      "Getting transcript for i81EK2x5fEE\n",
      "Getting transcript for 3CJWYrSzI6E\n",
      "Getting transcript for 6GTjVl9a47Y\n",
      "Getting transcript for IFiJ4YlSYt0\n",
      "Getting transcript for HXgMLpc7ivE\n",
      "Getting transcript for L1XhvNZ1eGo\n"
     ]
    }
   ],
   "source": [
    "keywords = 'how do i get my annual free credit report'\n",
    "print('Working on keywords: ' + keywords)\n",
    "video_list = getBestVideoList(keywords, 30)\n",
    "# video_list = [{'id': {'videoId': 'kqMtDrsc5Pw'}}]\n",
    "transcript_list = []\n",
    "for video in video_list:\n",
    "    try: \n",
    "        transcript_list.append(getTranscript(video['id']['videoId']))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "content_list = []\n",
    "for transcript in transcript_list:\n",
    "    content = '. '.join(list(map(lambda x: x['text'], transcript)))\n",
    "    # print(content)\n",
    "    # print(\"\\n\\n\")\n",
    "    content_list.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/611 [00:00<?, ?it/s]# Unique phrases: 1615\n",
      "# phrases: 611\n",
      "Fatal error for true financial\n",
      "Fatal error for investing and retirement\n",
      "Fatal error for new videos\n",
      "  3%|▎         | 16/611 [00:00<00:19, 29.94it/s]Fatal error for home loan\n",
      "Fatal error for delinquent payment\n",
      "Fatal error for financial future\n",
      "Fatal error for different companies\n",
      "  4%|▎         | 22/611 [00:00<00:20, 29.38it/s]Fatal error for huge headaches\n",
      "Fatal error for young adults\n",
      "  5%|▍         | 28/611 [00:00<00:20, 29.12it/s]Fatal error for four month plan\n",
      "Fatal error for absent services\n",
      "Fatal error for official website\n",
      "  6%|▌         | 35/611 [00:01<00:19, 30.08it/s]Fatal error for first home\n",
      "Fatal error for rental property\n",
      "Fatal error for calvin russell\n",
      "Fatal error for free credit\n",
      "  7%|▋         | 43/611 [00:01<00:18, 30.34it/s]Fatal error for x button\n",
      "Fatal error for first problem\n",
      "  8%|▊         | 51/611 [00:01<00:18, 30.08it/s]Fatal error for personal statement\n",
      "Fatal error for phone numbers\n",
      "Fatal error for exact month\n",
      "Fatal error for last activity\n",
      " 10%|█         | 63/611 [00:02<00:21, 26.09it/s]Fatal error for full complete picture\n",
      "Fatal error for twenty thirty dollars\n",
      "Fatal error for great day\n",
      " 12%|█▏        | 72/611 [00:02<00:21, 24.85it/s]Fatal error for important time\n",
      " 13%|█▎        | 78/611 [00:02<00:20, 25.41it/s]Fatal error for previous addresses\n",
      " 14%|█▎        | 84/611 [00:02<00:19, 27.50it/s]Fatal error for next report\n",
      "Fatal error for equifax section\n",
      "Fatal error for equifax report\n",
      "Fatal error for last four\n",
      " 15%|█▍        | 90/611 [00:03<00:20, 25.31it/s]Fatal error for free experian\n",
      "Fatal error for eight new alerts\n",
      "Fatal error for fico score\n",
      " 16%|█▌        | 96/611 [00:03<00:25, 20.18it/s]Fatal error for vantage\n",
      "Fatal error for credit karma\n",
      "Fatal error for experian experian account\n",
      " 17%|█▋        | 102/611 [00:03<00:23, 21.52it/s]Fatal error for remzi journey\n",
      "Fatal error for refreshed\n",
      " 18%|█▊        | 108/611 [00:04<00:23, 21.18it/s]Fatal error for seven days\n",
      "Fatal error for updated credit\n",
      "Fatal error for equifax experian\n",
      "Fatal error for another website\n",
      "Fatal error for demographic information\n",
      " 18%|█▊        | 111/611 [00:04<00:23, 21.54it/s]Fatal error for wrong hands\n",
      "Fatal error for every report\n",
      "Fatal error for february 14th\n",
      " 19%|█▉        | 117/611 [00:04<00:24, 19.87it/s]Fatal error for october 14th\n",
      "Fatal error for new report\n",
      "Fatal error for alphabetical order\n",
      "Fatal error for june 4\n",
      " 20%|██        | 125/611 [00:05<00:24, 19.69it/s]Fatal error for certain account\n",
      "Fatal error for secure location\n",
      " 21%|██▏       | 131/611 [00:05<00:26, 17.85it/s]Fatal error for active accounts\n",
      "Fatal error for every active account\n",
      "Fatal error for payment amount\n",
      " 23%|██▎       | 141/611 [00:05<00:24, 19.01it/s]Fatal error for certain expectations\n",
      "Fatal error for self credit\n",
      " 24%|██▍       | 149/611 [00:06<00:24, 18.71it/s]Fatal error for best version\n",
      "Fatal error for actual details\n",
      "Fatal error for good credit\n",
      "Fatal error for steve stewart\n",
      " 25%|██▌       | 154/611 [00:06<00:23, 19.08it/s]Fatal error for one report\n",
      "Fatal error for free report\n",
      " 26%|██▌       | 160/611 [00:06<00:26, 17.07it/s]Fatal error for captcha\n",
      " 27%|██▋       | 168/611 [00:07<00:24, 18.15it/s]Fatal error for google calendar\n",
      "Fatal error for aligner\n",
      " 28%|██▊       | 174/611 [00:07<00:26, 16.57it/s]Fatal error for credit bureau\n",
      "Fatal error for transunion experian\n",
      " 29%|██▉       | 178/611 [00:08<00:30, 14.12it/s]Fatal error for next screen\n",
      "Fatal error for verisign\n",
      " 29%|██▉       | 180/611 [00:08<00:31, 13.60it/s]Fatal error for third one\n",
      " 30%|███       | 184/611 [00:08<00:33, 12.89it/s]Fatal error for trans union\n",
      "Fatal error for first five\n",
      "Fatal error for next page\n",
      " 31%|███       | 188/611 [00:08<00:30, 13.74it/s]Fatal error for online report\n",
      "Fatal error for another chance\n",
      "Fatal error for new card\n",
      " 31%|███       | 190/611 [00:08<00:31, 13.16it/s]Fatal error for three other options\n",
      "Fatal error for original website\n",
      " 32%|███▏      | 194/611 [00:09<00:33, 12.32it/s]Fatal error for next april\n",
      " 33%|███▎      | 202/611 [00:09<00:30, 13.62it/s]Fatal error for three different ways\n",
      "Fatal error for individual\n",
      "credit bureau\n",
      "Fatal error for your\n",
      "credit report request\n",
      "Fatal error for 15 days\n",
      " 34%|███▎      | 206/611 [00:10<00:26, 15.40it/s]Fatal error for three ways\n",
      "Fatal error for three methods\n",
      "Fatal error for specific website\n",
      " 34%|███▍      | 210/611 [00:10<00:25, 15.47it/s]Fatal error for your\n",
      "first name\n",
      "Fatal error for the\n",
      "next page\n",
      "Fatal error for a\n",
      "while\n",
      " 35%|███▌      | 214/611 [00:10<00:23, 16.58it/s]Fatal error for inquiries\n",
      "Fatal error for car dealership\n",
      "Fatal error for new home\n",
      " 36%|███▌      | 218/611 [00:10<00:25, 15.72it/s]Fatal error for kinda balance\n",
      "Fatal error for car loan\n",
      " 37%|███▋      | 224/611 [00:11<00:24, 15.64it/s]Fatal error for our\n",
      "information\n",
      "Fatal error for lifelock\n",
      " 38%|███▊      | 230/611 [00:11<00:21, 17.42it/s]Fatal error for house\n",
      "alert alarm system\n",
      "Fatal error for a\n",
      "credit monitoring system\n",
      "Fatal error for kinda services\n",
      "Fatal error for higher risk\n",
      " 38%|███▊      | 234/611 [00:11<00:22, 17.11it/s]Fatal error for public computer\n",
      " 39%|███▉      | 238/611 [00:12<00:22, 16.27it/s]Fatal error for home network\n",
      "Fatal error for free service\n",
      " 40%|███▉      | 242/611 [00:12<00:23, 15.80it/s]Fatal error for fraudulent website\n",
      "Fatal error for entire playlist\n",
      "Fatal error for trip astute\n",
      " 40%|████      | 246/611 [00:12<00:23, 15.57it/s]Fatal error for credit sesame\n",
      "Fatal error for major changes\n",
      "Fatal error for mandated website\n",
      " 41%|████      | 250/611 [00:12<00:22, 15.87it/s]Fatal error for other\n",
      "places\n",
      "Fatal error for recent history\n",
      "Fatal error for entire credit report\n",
      " 42%|████▏     | 254/611 [00:13<00:22, 15.81it/s]Fatal error for authorized user\n",
      "Fatal error for a\n",
      "negative way\n",
      "Fatal error for free annual\n",
      "Fatal error for your\n",
      "information\n",
      " 42%|████▏     | 258/611 [00:13<00:22, 15.37it/s]Fatal error for past address\n",
      "Fatal error for any\n",
      "issues\n",
      "Fatal error for annual credit\n",
      "report\n",
      "Fatal error for mail or phone\n",
      " 43%|████▎     | 260/611 [00:13<00:25, 13.68it/s]Fatal error for the\n",
      "request form\n",
      "Fatal error for the\n",
      "corrections\n",
      " 43%|████▎     | 264/611 [00:13<00:24, 14.30it/s]Fatal error for right site\n",
      "Fatal error for accidental traffic\n",
      "Fatal error for extra services\n",
      " 44%|████▎     | 266/611 [00:14<00:24, 13.99it/s]Fatal error for up-sell\n",
      "Fatal error for authentication process\n",
      "Fatal error for transunion credit\n",
      "report\n",
      " 44%|████▍     | 270/611 [00:14<00:23, 14.29it/s]Fatal error for experian\n",
      "and equifax\n",
      "Fatal error for different questions\n",
      "Fatal error for additional reports\n",
      " 45%|████▍     | 274/611 [00:14<00:22, 14.96it/s]Fatal error for taskrabbit\n",
      "Fatal error for common error\n",
      "Fatal error for out-of-date employment information\n",
      " 45%|████▌     | 278/611 [00:14<00:20, 16.54it/s]Fatal error for annual fees\n",
      "Fatal error for simple tracker\n",
      " 46%|████▌     | 282/611 [00:15<00:23, 13.84it/s]Fatal error for the\n",
      "annual credit report website\n",
      "Fatal error for standardized process\n",
      "Fatal error for three unique and distinct\n",
      "businesses\n",
      " 47%|████▋     | 286/611 [00:15<00:23, 13.85it/s]Fatal error for one entity\n",
      "Fatal error for updates or corrections\n",
      " 47%|████▋     | 288/611 [00:15<00:24, 13.00it/s]Fatal error for discrepancies\n",
      "Fatal error for comment\n",
      "section\n",
      "Fatal error for newsletter\n",
      " 48%|████▊     | 292/611 [00:15<00:23, 13.37it/s]Fatal error for our\n",
      "website\n",
      "Fatal error for travel updates\n",
      " 48%|████▊     | 296/611 [00:16<00:22, 13.73it/s]Fatal error for federal mandate\n",
      "Fatal error for every 12 months\n",
      " 49%|████▉     | 298/611 [00:16<00:23, 13.29it/s]Fatal error for weekly credit\n",
      "Fatal error for every single week\n",
      " 50%|████▉     | 304/611 [00:16<00:21, 14.27it/s]Fatal error for websites and tv\n",
      "Fatal error for advertisements\n",
      "Fatal error for selected state\n",
      " 50%|█████     | 306/611 [00:16<00:21, 14.48it/s]Fatal error for red button unit\n",
      "Fatal error for one red cent\n",
      " 51%|█████     | 310/611 [00:17<00:20, 14.80it/s]Fatal error for last page\n",
      "Fatal error for entire process\n",
      "Fatal error for john harris\n",
      " 52%|█████▏    | 316/611 [00:17<00:20, 14.29it/s]Fatal error for special note\n",
      "Fatal error for full account numbers\n",
      " 53%|█████▎    | 322/611 [00:18<00:23, 12.25it/s]Fatal error for print report\n",
      " 53%|█████▎    | 324/611 [00:18<00:26, 10.68it/s]Fatal error for quick video\n",
      " 54%|█████▎    | 328/611 [00:18<00:25, 11.12it/s]Fatal error for equifax page\n",
      "Fatal error for first page\n",
      "Fatal error for continue button\n",
      " 54%|█████▍    | 330/611 [00:18<00:27, 10.38it/s]Fatal error for retort\n",
      "Fatal error for another screen\n",
      " 55%|█████▍    | 334/611 [00:19<00:28,  9.83it/s]Fatal error for free annual credit\n",
      "Fatal error for cool videos\n",
      " 55%|█████▌    | 338/611 [00:19<00:27,  9.96it/s]Fatal error for accuracy and fraud\n",
      "Fatal error for creditor\n",
      " 57%|█████▋    | 346/611 [00:20<00:20, 13.10it/s]Fatal error for andy webb\n",
      "Fatal error for important numbers\n",
      " 57%|█████▋    | 350/611 [00:20<00:21, 12.13it/s]Fatal error for misnomer\n",
      "Fatal error for mobile phones\n",
      " 58%|█████▊    | 354/611 [00:20<00:18, 13.88it/s]Fatal error for mortgages\n",
      "Fatal error for financial history\n",
      "Fatal error for free trials\n",
      " 59%|█████▊    | 358/611 [00:21<00:16, 14.90it/s]Fatal error for free services\n",
      "Fatal error for martin lewis\n",
      " 59%|█████▉    | 360/611 [00:21<00:22, 11.34it/s]Fatal error for little graph\n",
      " 60%|█████▉    | 364/611 [00:21<00:25,  9.71it/s]Fatal error for amanda christensen\n",
      "Fatal error for utah state\n",
      "university extension\n",
      " 60%|██████    | 368/611 [00:22<00:22, 10.62it/s]Fatal error for big red button\n",
      "Fatal error for free\n",
      "reports\n",
      "Fatal error for three simple steps\n",
      " 61%|██████    | 370/611 [00:22<00:22, 10.85it/s]Fatal error for great resources\n",
      "Fatal error for jargon\n",
      " 61%|██████    | 374/611 [00:22<00:19, 12.31it/s]Fatal error for regular monitoring\n",
      "Fatal error for description box\n",
      " 62%|██████▏   | 376/611 [00:22<00:21, 11.18it/s]Fatal error for utah state university\n",
      "extension\n",
      " 62%|██████▏   | 378/611 [00:23<00:22, 10.39it/s]Fatal error for financial life\n",
      "Fatal error for your\n",
      "scores\n",
      " 62%|██████▏   | 380/611 [00:23<00:22, 10.48it/s]Fatal error for five ways\n",
      "Fatal error for vantage\n",
      " 63%|██████▎   | 384/611 [00:23<00:20, 10.83it/s]Fatal error for many free\n",
      "websites\n",
      "Fatal error for a\n",
      "creditor\n",
      "Fatal error for fico and vantage scores\n",
      " 64%|██████▎   | 388/611 [00:23<00:20, 11.09it/s]Fatal error for 300 to 850\n",
      "Fatal error for clearer picture\n",
      "Fatal error for the\n",
      "difference\n",
      " 64%|██████▍   | 390/611 [00:24<00:19, 11.32it/s]Fatal error for convenient ways\n",
      "Fatal error for first way\n",
      "Fatal error for your\n",
      "fico score\n",
      " 64%|██████▍   | 394/611 [00:24<00:18, 11.73it/s]Fatal error for discover bank\n",
      "Fatal error for credit scorecard\n",
      "Fatal error for the\n",
      "service\n",
      " 65%|██████▍   | 396/611 [00:24<00:18, 11.43it/s]Fatal error for free account\n",
      "Fatal error for total number\n",
      " 65%|██████▌   | 398/611 [00:24<00:20, 10.51it/s]Fatal error for the\n",
      "calculation\n",
      "Fatal error for paid\n",
      "products\n",
      " 65%|██████▌   | 400/611 [00:25<00:21,  9.83it/s]Fatal error for second way\n",
      "Fatal error for free fico score\n",
      " 66%|██████▌   | 404/611 [00:25<00:22,  9.15it/s]Fatal error for online account\n",
      "Fatal error for the\n",
      "toll-free number\n",
      " 67%|██████▋   | 407/611 [00:25<00:23,  8.64it/s]Fatal error for third way\n",
      "Fatal error for bank or credit union\n",
      "Fatal error for financial institution\n",
      " 67%|██████▋   | 409/611 [00:26<00:21,  9.30it/s]Fatal error for our\n",
      "recommended ways\n",
      "Fatal error for popular and\n",
      "convenient ways\n",
      "Fatal error for team clark\n",
      " 68%|██████▊   | 413/611 [00:26<00:18, 10.42it/s]Fatal error for two favorites\n",
      "Fatal error for a\n",
      "free account\n",
      "Fatal error for two scores\n",
      " 68%|██████▊   | 415/611 [00:26<00:18, 10.85it/s]Fatal error for different\n",
      "creditors\n",
      "Fatal error for big\n",
      "reasons\n",
      "Fatal error for credit score simulator\n",
      " 69%|██████▊   | 419/611 [00:26<00:17, 10.88it/s]Fatal error for certain actions\n",
      "Fatal error for new loan\n",
      "Fatal error for new credit\n",
      " 69%|██████▉   | 421/611 [00:27<00:17, 10.74it/s]Fatal error for oldest credit card\n",
      " 69%|██████▉   | 423/611 [00:27<00:17, 10.51it/s]Fatal error for full\n",
      "credit karma review\n",
      "Fatal error for the\n",
      "second way\n",
      " 70%|██████▉   | 425/611 [00:27<00:18, 10.22it/s]Fatal error for good\n",
      "money and credit habits\n",
      "Fatal error for your\n",
      "financial life\n",
      " 70%|███████   | 429/611 [00:28<00:18, 10.07it/s]Fatal error for other\n",
      "videos\n",
      "Fatal error for money-saving\n",
      "information\n",
      "Fatal error for lance larsen\n",
      " 71%|███████   | 431/611 [00:28<00:18,  9.78it/s]Fatal error for chief security\n",
      "Fatal error for another tip\n",
      " 71%|███████   | 435/611 [00:28<00:21,  8.22it/s]Fatal error for credit\n",
      "guy tv\n",
      " 72%|███████▏  | 439/611 [00:29<00:19,  8.60it/s]Fatal error for free copy\n",
      "Fatal error for secondary address\n",
      " 72%|███████▏  | 441/611 [00:29<00:18,  9.19it/s]Fatal error for key factor\n",
      "Fatal error for three launching\n",
      "pads\n",
      " 73%|███████▎  | 447/611 [00:29<00:16,  9.70it/s]Fatal error for your\n",
      "credit history\n",
      "Fatal error for experian website\n",
      " 73%|███████▎  | 449/611 [00:30<00:16,  9.79it/s]Fatal error for annual\n",
      "credit report\n",
      "Fatal error for 4 security questions\n",
      " 74%|███████▍  | 451/611 [00:30<00:16,  9.61it/s]Fatal error for advanced logic\n",
      "Fatal error for main\n",
      "screen\n",
      "Fatal error for good accounts\n",
      " 74%|███████▍  | 453/611 [00:30<00:18,  8.76it/s]Fatal error for the\n",
      "my fico website\n",
      " 75%|███████▍  | 456/611 [00:30<00:16,  9.36it/s]Fatal error for two bureaus\n",
      "Fatal error for 3 credit bureaus\n",
      " 75%|███████▍  | 458/611 [00:31<00:17,  8.84it/s]Fatal error for next video\n",
      "Fatal error for credit industry\n",
      "Fatal error for free transunion credit\n",
      " 75%|███████▌  | 460/611 [00:31<00:17,  8.76it/s]Fatal error for free equifax credit\n",
      "Fatal error for laptop computer\n",
      " 76%|███████▌  | 465/611 [00:31<00:15,  9.30it/s]Fatal error for credit karma\n",
      "Fatal error for menu bar\n",
      " 77%|███████▋  | 468/611 [00:32<00:17,  8.25it/s]Fatal error for left hand side\n",
      "Fatal error for current credit\n",
      "Fatal error for cumulative reports\n",
      " 77%|███████▋  | 473/611 [00:32<00:15,  8.81it/s]Fatal error for new page\n",
      " 78%|███████▊  | 476/611 [00:33<00:14,  9.38it/s]Fatal error for printed credit\n",
      " 78%|███████▊  | 477/611 [00:33<00:15,  8.54it/s]Fatal error for free pdf\n",
      "Fatal error for drop-down option\n",
      " 79%|███████▊  | 480/611 [00:33<00:14,  9.26it/s]Fatal error for previous computers\n",
      "Fatal error for left-hand side\n",
      " 79%|███████▉  | 482/611 [00:33<00:13,  9.66it/s]Fatal error for computer ok\n",
      "Fatal error for every american\n",
      " 79%|███████▉  | 484/611 [00:33<00:12, 10.00it/s]Fatal error for credit com\n",
      " 80%|███████▉  | 486/611 [00:34<00:15,  7.84it/s]Fatal error for real important time\n",
      "Fatal error for coronavirus\n",
      " 80%|████████  | 489/611 [00:34<00:19,  6.19it/s]Fatal error for free score\n",
      " 81%|████████  | 492/611 [00:35<00:18,  6.48it/s]Fatal error for julian\n",
      "Fatal error for transunion and experian score\n",
      " 81%|████████  | 493/611 [00:35<00:17,  6.64it/s]Fatal error for little tricky um\n",
      "Fatal error for fico scores\n",
      " 82%|████████▏ | 498/611 [00:35<00:13,  8.50it/s]Fatal error for credit limits\n",
      "Fatal error for tough time\n",
      " 82%|████████▏ | 502/611 [00:36<00:11,  9.47it/s]Fatal error for coronavirus videos\n",
      "Fatal error for nice tips and tricks\n",
      " 82%|████████▏ | 503/611 [00:36<00:12,  8.98it/s]Fatal error for deferments\n",
      " 83%|████████▎ | 508/611 [00:37<00:13,  7.69it/s]Fatal error for 56 years\n",
      " 83%|████████▎ | 510/611 [00:37<00:13,  7.45it/s]Fatal error for culture country\n",
      " 84%|████████▍ | 512/611 [00:37<00:12,  8.15it/s]Fatal error for unprecedented times\n",
      "Fatal error for minimum monthly payments\n",
      "Fatal error for us bank\n",
      " 85%|████████▍ | 518/611 [00:38<00:12,  7.27it/s]Fatal error for awesome life\n",
      " 86%|████████▌ | 526/611 [00:39<00:12,  6.87it/s]Fatal error for credit information\n",
      "Fatal error for important step\n",
      " 87%|████████▋ | 531/611 [00:40<00:09,  8.02it/s]Fatal error for proper name\n",
      "Fatal error for various consumer databases\n",
      " 87%|████████▋ | 534/611 [00:40<00:08,  8.61it/s]Fatal error for pre-screen comm\n",
      " 88%|████████▊ | 536/611 [00:40<00:08,  8.91it/s]Fatal error for joint venture\n",
      " 88%|████████▊ | 539/611 [00:41<00:08,  8.73it/s]Fatal error for different sources\n",
      " 89%|████████▉ | 543/611 [00:41<00:07,  9.15it/s]Fatal error for credit builder loan\n",
      "Fatal error for credit builder loans\n",
      " 89%|████████▉ | 545/611 [00:41<00:07,  8.48it/s]Fatal error for credit sesame\n",
      "Fatal error for many companies\n",
      " 90%|████████▉ | 548/611 [00:42<00:06,  9.25it/s]Fatal error for either credit karma\n",
      "Fatal error for monitoring and score\n",
      " 90%|█████████ | 550/611 [00:42<00:06,  9.22it/s]Fatal error for different scoring model\n",
      " 91%|█████████ | 553/611 [00:42<00:07,  7.56it/s]Fatal error for main concern\n",
      "Fatal error for bureaus errors\n",
      " 91%|█████████ | 555/611 [00:43<00:08,  6.63it/s]Fatal error for huge differences\n",
      "Fatal error for potential red flag\n",
      " 91%|█████████ | 557/611 [00:43<00:07,  7.14it/s]Fatal error for lengthy history\n",
      " 91%|█████████▏| 559/611 [00:43<00:06,  7.91it/s]Fatal error for huge signup\n",
      " 92%|█████████▏| 562/611 [00:43<00:06,  7.19it/s]Fatal error for mediocre cards\n",
      "Fatal error for mediocre ones\n",
      " 92%|█████████▏| 563/611 [00:44<00:06,  7.03it/s]Fatal error for nuance credit\n",
      " 93%|█████████▎| 566/611 [00:44<00:06,  6.86it/s]Fatal error for scoring system\n",
      "Fatal error for free credit scores\n",
      "Fatal error for different lenders\n",
      " 93%|█████████▎| 569/611 [00:44<00:05,  7.63it/s]Fatal error for free scores\n",
      "Fatal error for score and scoring model\n",
      " 93%|█████████▎| 570/611 [00:44<00:05,  7.60it/s]Fatal error for great score\n",
      "Fatal error for real value\n",
      " 94%|█████████▍| 573/611 [00:45<00:05,  7.07it/s]Fatal error for recent applications\n",
      " 94%|█████████▍| 574/611 [00:45<00:05,  7.17it/s]Fatal error for identity theft alert\n",
      " 95%|█████████▍| 578/611 [00:46<00:04,  7.25it/s]Fatal error for experian and credit\n",
      "Fatal error for credit\n",
      "guy tv\n",
      " 95%|█████████▌| 582/611 [00:46<00:03,  7.98it/s]Fatal error for collection letter\n",
      "Fatal error for 6 months\n",
      " 96%|█████████▌| 585/611 [00:46<00:03,  8.43it/s]Fatal error for honest answer\n",
      "Fatal error for the\n",
      "information\n",
      " 96%|█████████▌| 587/611 [00:47<00:02,  8.26it/s]Fatal error for recent collection\n",
      "Fatal error for 6\n",
      "months\n",
      " 97%|█████████▋| 590/611 [00:47<00:02,  8.88it/s]Fatal error for collection company\n",
      "Fatal error for new reporting date\n",
      " 97%|█████████▋| 592/611 [00:47<00:02,  9.40it/s]Fatal error for big mistake\n",
      "Fatal error for the\n",
      "verification\n",
      "Fatal error for debt letter\n",
      " 98%|█████████▊| 596/611 [00:48<00:01, 10.60it/s]Fatal error for financial\n",
      "house\n",
      "Fatal error for one\n",
      "free copy\n",
      "Fatal error for important information\n",
      " 98%|█████████▊| 600/611 [00:48<00:01, 10.67it/s]Fatal error for financial accounts\n",
      " 99%|█████████▊| 602/611 [00:48<00:00, 10.72it/s]Fatal error for free\n",
      "credit\n",
      "Fatal error for your\n",
      "name\n",
      " 99%|█████████▉| 605/611 [00:48<00:00,  9.34it/s]Fatal error for one free copy\n",
      "Fatal error for annual report\n",
      " 99%|█████████▉| 607/611 [00:49<00:00,  8.49it/s]Fatal error for four ways\n",
      "Fatal error for part 2\n",
      "100%|█████████▉| 608/611 [00:49<00:00,  8.70it/s]Fatal error for separate video\n",
      "100%|█████████▉| 610/611 [00:49<00:00,  6.51it/s]Fatal error for fastest way\n",
      "100%|██████████| 611/611 [00:49<00:00, 12.27it/s]611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# keyPhrases, entities = getKeyPhrases(content_list)\n",
    "keyPhrases = getKeyPhrases(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "611\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "total_words = 0\n",
    "def insertTfidf(phrase):\n",
    "    # 1.4 M books\n",
    "    # phrase['tfidf'] = (phrase['frequency']['overall'] / total_words) * (1+math.log(1180278/(1+phrase['ngram_volume_count'])))\n",
    "    phrase['tfidf'] = math.log((phrase['frequency']['overall']+1)) * (1+math.log(1180278/(1+phrase['ngram_volume_count'])))\n",
    "    # phrase['tfidf'] = (1) * (1+math.log(1180278/(1+phrase['ngram_volume_count'])))\n",
    "    return phrase\n",
    "\n",
    "def flatten(phrases):\n",
    "    rv = []\n",
    "    for phrase in phrases:\n",
    "        rv.append({\n",
    "            'phrase': phrase['Text'],\n",
    "            'ori_text': phrase['ori_text'],\n",
    "            'overall_freq': phrase['frequency']['overall'],\n",
    "            'min_freq': phrase['frequency']['min'],\n",
    "            'max_freq': phrase['frequency']['max'],\n",
    "            'median_freq': phrase['frequency']['median'],\n",
    "            'ngram_volume_count': phrase['ngram_volume_count'],\n",
    "            'ngram_match_count': phrase['ngram_match_count'],\n",
    "            'tfidf': phrase['tfidf']\n",
    "        })\n",
    "    return rv\n",
    "\n",
    "def printTable(phrases):\n",
    "    table = PrettyTable()\n",
    "    table.field_names=['phrase', 'ori_text', 'overall_freq', 'min_freq', 'max_freq', 'median_freq', 'doc_freq', 'ngram_volume_count', 'ngram_match_count', 'tfidf', 'entity_type']\n",
    "    for phrase in phrases:\n",
    "        entity_type = 'N/A'\n",
    "        if 'Type' in phrase:\n",
    "            entity_type = phrase['Type']\n",
    "        table.add_row([phrase['Text'], phrase['ori_text'], phrase['Score'], phrase['frequency']['overall'], phrase['frequency']['min'], phrase['frequency']['max'], phrase['frequency']['median'], phrase['frequency']['doc_freq'], phrase['ngram_volume_count'], phrase['ngram_match_count'], phrase['tfidf'], entity_type])\n",
    "    print(table)\n",
    "\n",
    "for content in content_list:\n",
    "    total_words += len(content.split())\n",
    "\n",
    "print(len(keyPhrases))\n",
    "keyPhrases = list(map(insertTfidf, keyPhrases))\n",
    "#print top keyphrases table\n",
    "keyPhrases.sort(reverse=True, key=lambda x: x['tfidf'])\n",
    "flattenKeyPhrases = flatten(keyPhrases)\n",
    "# printTable(keyPhrases)\n",
    "#printTable(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}