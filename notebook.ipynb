{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from functools import reduce\n",
    "import pprint\n",
    "from prettytable import PrettyTable\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "from phrasefinder import phrasefinder as pf\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "\n",
    "comprehend = boto3.client(service_name='comprehend', region_name='us-east-1')\n",
    "\n",
    "def getBestVideoList(keywords):\n",
    "    scopes = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"0\"\n",
    "\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    client_secrets_file = \"./google_credentials/client_secret_754636752811-rmth1g8e3dl144jda8fddh1ihhj413um.apps.googleusercontent.com.json\"\n",
    "\n",
    "    # Get credentials and create an API client\n",
    "    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n",
    "        client_secrets_file, scopes)\n",
    "\n",
    "    credentials =Credentials(\n",
    "        None,\n",
    "        refresh_token=\"1//0fNppFYz3o7ABCgYIARAAGA8SNwF-L9IrgIZJAKCn9iSH_172SxyT6cA3mMHDlSQ0MTj9MmKTc6zZRnSy1nwMW5kRkl52JYb4jhg\",\n",
    "        token_uri=\"https://accounts.google.com/o/oauth2/token\",\n",
    "        client_id=\"754636752811-rmth1g8e3dl144jda8fddh1ihhj413um.apps.googleusercontent.com\",\n",
    "        client_secret=\"KhUufHmhS8XI0srgpP__cTCr\"\n",
    "    )\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, credentials=credentials)\n",
    "\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        maxResults=10,\n",
    "        q=keywords,\n",
    "        relevanceLanguage='en'\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items']\n",
    "\n",
    "\"\"\"     best_content_list = []\n",
    "    with os.scandir('./data') as entries:\n",
    "        for entry in entries:\n",
    "            with open('./data/' + entry.name, 'r') as file:\n",
    "                best_content_list.append(file.read())\n",
    "    return best_content_list \"\"\"\n",
    "\n",
    "def getKeyPhrases(content_list, top):\n",
    "    def insertFrequency(keyPhrase):\n",
    "        frequency = {'overall': 0, 'min': 0, 'max': 0, 'frequency_by_content': []}\n",
    "        for content in content_list:\n",
    "            _freq = content.count(keyPhrase['Text'])\n",
    "            frequency['overall'] += _freq\n",
    "            frequency['min'] = _freq if frequency['min'] == 0 else min(frequency['min'], _freq)\n",
    "            frequency['max'] = max(frequency['max'], _freq)\n",
    "            frequency['frequency_by_content'].append(_freq)\n",
    "        return {**keyPhrase, 'frequency': frequency}\n",
    "\n",
    "    def filterPhrases(phrasesByContent, top):\n",
    "        phrases = reduce(lambda x,y: x+y, phrasesByContent)\n",
    "        phrases = list(filter(lambda x: x['Score'] >= 0.995, phrases))\n",
    "\n",
    "        # trim stop words from begining and end\n",
    "        def stripStopWords(phrase):\n",
    "            for w in stop_words.ENGLISH_STOP_WORDS:\n",
    "                if phrase['Text'].startswith(w):\n",
    "                    word = word[len(w):]\n",
    "                    break\n",
    "                elif word.endswith(w):\n",
    "                    word = word[:-len(w)]\n",
    "            return word\n",
    "        phrases = list(map(lambda x: x['Text'] = stripStopWords, phrases))\n",
    "\n",
    "        # only keep bigrams\n",
    "        phrases = list(filter(lambda x: len(x['Text'].split(' ')) >= 2, phrases))\n",
    "\n",
    "        # remove duplicates\n",
    "        unique_phrases=[]\n",
    "        phrase_set = set()\n",
    "        for phrase in phrases:\n",
    "            if phrase['Text'] not in phrase_set:\n",
    "                phrase_set.add(phrase['Text'])\n",
    "                unique_phrases.append(phrase)\n",
    "\n",
    "        # add frequency for each phrase\n",
    "        unique_phrases = list(map(insertFrequency, unique_phrases))\n",
    "        phrases = list(map(insertFrequency, unique_phrases))\n",
    "\n",
    "        # sort key phrases by overall frequency\n",
    "        phrases.sort(reverse=True, key=lambda x: x['frequency']['overall'])\n",
    "\n",
    "        print(len(phrases))\n",
    "\n",
    "        # return most frequent top phrases\n",
    "        return phrases[0:top]\n",
    "\n",
    "    # comprehend api requires each content to be less than 5000 bytes\n",
    "    # see https://docs.aws.amazon.com/comprehend/latest/dg/guidelines-and-limits.html\n",
    "    content_list_splited = []\n",
    "    for content in content_list:\n",
    "        content_list_splited.extend([content[i: i+4000] for i in range(0, len(content), 4000)])\n",
    "    \"\"\" for content in content_list_splited:\n",
    "        print(len(content))\n",
    " \"\"\"\n",
    "    def chunks(l, n):\n",
    "        for i in range(0, len(l), n):\n",
    "         yield l[i:i + n]\n",
    "\n",
    "    content_list_splited_chunks = list(chunks(content_list_splited, 25))\n",
    "\n",
    "    keyPhrasesByContent = []\n",
    "    entitiesByContent = []\n",
    "    for chunk in content_list_splited_chunks:\n",
    "        keyPhrasesResponse = comprehend.batch_detect_key_phrases(TextList=chunk, LanguageCode='en')\n",
    "        entitiesResponse = comprehend.batch_detect_entities(TextList=chunk, LanguageCode='en')\n",
    "\n",
    "        keyPhrasesByContent.extend(list(map(lambda x: x['KeyPhrases'], keyPhrasesResponse['ResultList'])))\n",
    "        entitiesByContent.extend(list(map(lambda x: x['Entities'], entitiesResponse['ResultList'])))\n",
    "\n",
    "    return filterPhrases(keyPhrasesByContent, top), filterPhrases(entitiesByContent, top)\n",
    "\n",
    "def getTranscript(video_id):\n",
    "    print('Getting transcript for '+video_id)\n",
    "    return YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "def printTable(phrases):\n",
    "    table = PrettyTable()\n",
    "    table.field_names=['phrase', 'score', 'overall_freq', 'min_freq', 'max_freq', 'entity_type']\n",
    "    for phrase in phrases:\n",
    "        entity_type = 'N/A'\n",
    "        if 'Type' in phrase:\n",
    "            entity_type = phrase['Type']\n",
    "        table.add_row([phrase['Text'], phrase['Score'], phrase['frequency']['overall'], phrase['frequency']['min'], phrase['frequency']['max'], entity_type])\n",
    "    print(table)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Working on keywords: How To Buy Your First Rental\n",
      "Getting transcript for bJx7_1rWC6U\n",
      "Getting transcript for 7TB_eRhSNV4\n",
      "Getting transcript for u83O2l1QEj4\n",
      "Getting transcript for LxAniFgeCOg\n",
      "Getting transcript for IOqlt3dwb_c\n",
      "Getting transcript for a5RExfeU4UY\n",
      "Getting transcript for qiKg3790dtA\n",
      "Getting transcript for nb9gtT-BqRc\n",
      "Getting transcript for CPJp9Ch37UM\n",
      "Getting transcript for zoCGqNkBsbA\n"
     ]
    }
   ],
   "source": [
    "keywords = 'How To Buy Your First Rental'\n",
    "print('Working on keywords: ' + keywords)\n",
    "video_list = getBestVideoList(keywords)\n",
    "# video_list = [{'id': {'videoId': 'kqMtDrsc5Pw'}}]\n",
    "transcript_list = []\n",
    "for video in video_list:\n",
    "    try: \n",
    "        transcript_list.append(getTranscript(video['id']['videoId']))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "content_list = []\n",
    "for transcript in transcript_list:\n",
    "    content = '. '.join(list(map(lambda x: x['text'], transcript)))\n",
    "    # print(content)\n",
    "    # print(\"\\n\\n\")\n",
    "    content_list.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'startswith'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-65ced0ce36ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeyPhrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetKeyPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyPhrases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print top keyphrases table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprintTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyPhrases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#printTable(entities)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-e26572786893>\u001b[0m in \u001b[0;36mgetKeyPhrases\u001b[0;34m(content_list, top)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mentitiesByContent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentitiesResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ResultList'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilterPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyPhrasesByContent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentitiesByContent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetTranscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-e26572786893>\u001b[0m in \u001b[0;36mfilterPhrases\u001b[0;34m(phrasesByContent, top)\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mphrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstripStopWords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# only keep bigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-e26572786893>\u001b[0m in \u001b[0;36mstripStopWords\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstripStopWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "keyPhrases, entities = getKeyPhrases(content_list, 300)\n",
    "print(len(keyPhrases))\n",
    "# print top keyphrases table\n",
    "printTable(keyPhrases)\n",
    "#printTable(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}