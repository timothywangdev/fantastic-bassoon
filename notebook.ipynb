{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from functools import reduce\n",
    "import pprint\n",
    "from prettytable import PrettyTable\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "from phrasefinder import phrasefinder as pf\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "from functools import reduce\n",
    "from tinydb import TinyDB, Query\n",
    "\n",
    "comprehend = boto3.client(service_name='comprehend', region_name='us-east-1')\n",
    "\n",
    "def getBestVideoList(keywords, topn=10):\n",
    "    scopes = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"0\"\n",
    "\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    client_secrets_file = \"./google_credentials/client_secret_754636752811-rmth1g8e3dl144jda8fddh1ihhj413um.apps.googleusercontent.com.json\"\n",
    "\n",
    "    # Get credentials and create an API client\n",
    "    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n",
    "        client_secrets_file, scopes)\n",
    "\n",
    "    credentials =Credentials(\n",
    "        None,\n",
    "        refresh_token=\"1//0fNppFYz3o7ABCgYIARAAGA8SNwF-L9IrgIZJAKCn9iSH_172SxyT6cA3mMHDlSQ0MTj9MmKTc6zZRnSy1nwMW5kRkl52JYb4jhg\",\n",
    "        token_uri=\"https://accounts.google.com/o/oauth2/token\",\n",
    "        client_id=\"754636752811-rmth1g8e3dl144jda8fddh1ihhj413um.apps.googleusercontent.com\",\n",
    "        client_secret=\"KhUufHmhS8XI0srgpP__cTCr\"\n",
    "    )\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, credentials=credentials)\n",
    "\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        maxResults=topn,\n",
    "        q=keywords,\n",
    "        relevanceLanguage='en'\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items']\n",
    "\n",
    "\"\"\"     best_content_list = []\n",
    "    with os.scandir('./data') as entries:\n",
    "        for entry in entries:\n",
    "            with open('./data/' + entry.name, 'r') as file:\n",
    "                best_content_list.append(file.read())\n",
    "    return best_content_list \"\"\"\n",
    "\n",
    "def getKeyPhrases(content_list):\n",
    "    def insertFrequency(keyPhrase):\n",
    "        frequency = {'overall': 0, 'min': 0, 'max': 0, 'frequency_by_content': []}\n",
    "        for content in content_list:\n",
    "            _freq = content.count(keyPhrase['Text'])\n",
    "            frequency['overall'] += _freq\n",
    "            frequency['min'] = _freq if frequency['min'] == 0 else min(frequency['min'], _freq)\n",
    "            frequency['max'] = max(frequency['max'], _freq)\n",
    "            frequency['frequency_by_content'].append(_freq)\n",
    "\n",
    "        frequency['doc_freq'] = 0\n",
    "        for e in frequency['frequency_by_content']:\n",
    "            frequency['doc_freq'] += e>0\n",
    "\n",
    "        frequency['median'] = statistics.median(frequency['frequency_by_content'])\n",
    "        return {**keyPhrase, 'frequency': frequency}\n",
    "\n",
    "    def insertNgramFrequency(keyPhrase):\n",
    "\n",
    "        db = TinyDB('db.json')\n",
    "        ngramQuery = Query()\n",
    "\n",
    "         # Set up your query.\n",
    "        query = keyPhrase['Text']\n",
    "\n",
    "        ngramData = db.get(ngramQuery.text == query)\n",
    "\n",
    "        if ngramData is None:\n",
    "            ngramData = { 'text': query }\n",
    "\n",
    "            # Optional: set the maximum number of phrases to return.\n",
    "            options = pf.SearchOptions()\n",
    "            options.topk = 1\n",
    "\n",
    "            # Send the request.\n",
    "            try:\n",
    "                result = pf.search(pf.Corpus.AMERICAN_ENGLISH, query, options)\n",
    "                if result.error:\n",
    "                    print('Request for {} was not successful: {}'.format(query, result.error['message']))\n",
    "                    return\n",
    "  \n",
    "                time.sleep(0.02)\n",
    "                ngramData={**ngramData, 'volume_count': result.phrases[0].volume_count, 'match_count': result.phrases[0].match_count}\n",
    "            except Exception as error:\n",
    "                print('Fatal error for {}: {}'.format(query, error))\n",
    "                ngramData={**ngramData, 'volume_count': 0, 'match_count': 0}\n",
    "            \n",
    "            # cache ngramData\n",
    "            db.insert(ngramData)\n",
    "        \n",
    "        \n",
    "        return {**keyPhrase, 'ngram_volume_count': ngramData['volume_count'], 'ngram_match_count': ngramData['match_count']}\n",
    "\n",
    "    def filterPhrases(phrasesByContent):\n",
    "        phrases = reduce(lambda x,y: x+y, phrasesByContent)                       \n",
    "        # trim stop words from begining and end\n",
    "        def stripStopWords(phrase):\n",
    "            _stop_words = list(stop_words.ENGLISH_STOP_WORDS) + ['he', 'she', 'you', 'me', 'guy', 'guys']\n",
    "            while True:\n",
    "                # repetitively trim stop words from the beginning and the end\n",
    "                modified = False\n",
    "                for w in _stop_words:\n",
    "                    if phrase['Text'].startswith(w+' '):\n",
    "                        phrase['Text'] = phrase['Text'][len(w)+1:]\n",
    "                        modified = True\n",
    "                        break\n",
    "                    elif phrase['Text'].endswith(' '+w):\n",
    "                        phrase['Text'] = phrase['Text'][:-len(w)-1]\n",
    "                        modified = True\n",
    "                        break\n",
    "                    elif phrase['Text'] == w:\n",
    "                        phrase['Text'] = \"\"\n",
    "                        break\n",
    "                if not modified:\n",
    "                    break\n",
    "            return phrase\n",
    "\n",
    "        phrases = list(map(stripStopWords, phrases))\n",
    "\n",
    "        # only keep bigrams\n",
    "        phrases = list(filter(lambda x: len(x['Text'].split(' ')) >= 1, phrases))\n",
    "\n",
    "        # remove duplicates\n",
    "        unique_phrases=[]\n",
    "        phrase_set = set()\n",
    "        for phrase in phrases:\n",
    "            if phrase['Text'] not in phrase_set:\n",
    "                phrase_set.add(phrase['Text'])\n",
    "                unique_phrases.append(phrase)\n",
    "\n",
    "        print('# Unique phrases: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # add frequency for each phrase\n",
    "        unique_phrases = list(map(insertFrequency, unique_phrases))\n",
    "\n",
    "        # keep phrases showing up at least once in all documents\n",
    "        unique_phrases = list(filter(lambda x: x['frequency']['min'] >= 0, unique_phrases))\n",
    "\n",
    "        # phrase should have at least 3 chars\n",
    "        unique_phrases = list(filter(lambda x: len(x['Text']) >= 3, unique_phrases))\n",
    "\n",
    "        # phrase should not have weird symbols\n",
    "        unique_phrases = list(filter(lambda x: all(x.isalpha() or x.isspace() or x == '-' or x =='$' or x.isdigit() for x in x['Text']), unique_phrases))\n",
    "\n",
    "        print('# phrases: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # add ngrams frequency for each phrase\n",
    "        for i in tqdm(range(len(unique_phrases))):\n",
    "            unique_phrases[i] = insertNgramFrequency(unique_phrases[i])\n",
    "        #unique_phrases = list(map(insertNgramFrequency, unique_phrases))\n",
    "\n",
    "        phrases = unique_phrases\n",
    "\n",
    "        # sort key phrases by overall frequency\n",
    "        phrases.sort(reverse=True, key=lambda x: x['frequency']['overall'])\n",
    "\n",
    "        print(len(phrases))\n",
    "\n",
    "        # return most frequent top phrases\n",
    "        return phrases\n",
    "\n",
    "    # comprehend api requires each content to be less than 5000 bytes\n",
    "    # see https://docs.aws.amazon.com/comprehend/latest/dg/guidelines-and-limits.html\n",
    "    content_list_splited = []\n",
    "    for content in content_list:\n",
    "        content_list_splited.extend([content[i: i+4000] for i in range(0, len(content), 4000)])\n",
    "    \"\"\" for content in content_list_splited:\n",
    "        print(len(content))\n",
    " \"\"\"\n",
    "    def chunks(l, n):\n",
    "        for i in range(0, len(l), n):\n",
    "         yield l[i:i + n]\n",
    "\n",
    "    content_list_splited_chunks = list(chunks(content_list_splited, 25))\n",
    "\n",
    "    keyPhrasesByContent = []\n",
    "    entitiesByContent = []\n",
    "    for chunk in content_list_splited_chunks:\n",
    "        keyPhrasesResponse = comprehend.batch_detect_key_phrases(TextList=chunk, LanguageCode='en')\n",
    "        entitiesResponse = comprehend.batch_detect_entities(TextList=chunk, LanguageCode='en')\n",
    "\n",
    "        keyPhrasesByContent.extend(list(map(lambda x: x['KeyPhrases'], keyPhrasesResponse['ResultList'])))\n",
    "        entitiesByContent.extend(list(map(lambda x: x['Entities'], entitiesResponse['ResultList'])))\n",
    "\n",
    "    # return filterPhrases(keyPhrasesByContent), filterPhrases(entitiesByContent)\n",
    "    return filterPhrases(keyPhrasesByContent)\n",
    "\n",
    "def getTranscript(video_id):\n",
    "    print('Getting transcript for '+video_id)\n",
    "    return YouTubeTranscriptApi.get_transcript(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on keywords: How To Buy Your First Rental\n",
      "Getting transcript for bJx7_1rWC6U\n",
      "Getting transcript for 7TB_eRhSNV4\n",
      "Getting transcript for u83O2l1QEj4\n",
      "Getting transcript for nb9gtT-BqRc\n",
      "Getting transcript for IOqlt3dwb_c\n",
      "Getting transcript for LxAniFgeCOg\n",
      "Getting transcript for qiKg3790dtA\n",
      "Getting transcript for zoCGqNkBsbA\n",
      "Getting transcript for a5RExfeU4UY\n",
      "Getting transcript for yv4ZIaiRMZM\n"
     ]
    }
   ],
   "source": [
    "keywords = 'How To Buy Your First Rental'\n",
    "print('Working on keywords: ' + keywords)\n",
    "video_list = getBestVideoList(keywords)\n",
    "# video_list = [{'id': {'videoId': 'kqMtDrsc5Pw'}}]\n",
    "transcript_list = []\n",
    "for video in video_list:\n",
    "    try: \n",
    "        transcript_list.append(getTranscript(video['id']['videoId']))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "content_list = []\n",
    "for transcript in transcript_list:\n",
    "    content = '. '.join(list(map(lambda x: x['text'], transcript)))\n",
    "    # print(content)\n",
    "    # print(\"\\n\\n\")\n",
    "    content_list.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keyPhrases, entities = getKeyPhrases(content_list)\n",
    "keyPhrases = getKeyPhrases(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "total_words = 0\n",
    "def insertTfidf(phrase):\n",
    "    # 1.4 M books\n",
    "    phrase['tfidf'] = (phrase['frequency']['overall'] / total_words) * (1+math.log(1300000/(1+phrase['ngram_volume_count'])))\n",
    "    return phrase\n",
    "\n",
    "def printTable(phrases):\n",
    "    table = PrettyTable()\n",
    "    table.field_names=['phrase', 'ori_text', 'score', 'overall_freq', 'min_freq', 'max_freq', 'median_freq', 'doc_freq', 'ngram_volume_count', 'ngram_match_count', 'tfidf', 'entity_type']\n",
    "    for phrase in phrases:\n",
    "        entity_type = 'N/A'\n",
    "        if 'Type' in phrase:\n",
    "            entity_type = phrase['Type']\n",
    "        table.add_row([phrase['Text'], phrase['ori_text'], phrase['Score'], phrase['frequency']['overall'], phrase['frequency']['min'], phrase['frequency']['max'], phrase['frequency']['median'], phrase['frequency']['doc_freq'], phrase['ngram_volume_count'], phrase['ngram_match_count'], phrase['tfidf'], entity_type])\n",
    "    print(table)\n",
    "\n",
    "for content in content_list:\n",
    "    total_words += len(content.split())\n",
    "\n",
    "print(len(keyPhrases))\n",
    "keyPhrases = list(map(insertTfidf, keyPhrases))\n",
    "# print top keyphrases table\n",
    "keyPhrases.sort(reverse=True, key=lambda x: x['tfidf'])\n",
    "printTable(keyPhrases)\n",
    "#printTable(entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}