{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prettytable'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-72c5348801ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprettytable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myoutube_transcript_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYouTubeTranscriptApi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgoogle_auth_oauthlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prettytable'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from functools import reduce\n",
    "import pprint\n",
    "from prettytable import PrettyTable\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "from phrasefinder import phrasefinder as pf\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "from functools import reduce\n",
    "from tinydb import TinyDB, Query\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient\n",
    "from google.cloud.bigquery_storage import types\n",
    "from google.cloud import bigquery\n",
    "# from google.cloud import storage\n",
    "import re\n",
    "import os \n",
    "import sqlite3\n",
    "comprehend = boto3.client(service_name='comprehend', region_name='us-east-1')\n",
    "\n",
    "\n",
    "bigquery_client = None\n",
    "bigquery_data = None\n",
    "sql_conn = None\n",
    "\n",
    "def getBestVideoList(keywords, topn=10):\n",
    "    scopes = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"0\"\n",
    "\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    client_secrets_file = \"./google_credentials/client_secret_754636752811-rmth1g8e3dl144jda8fddh1ihhj413um.apps.googleusercontent.com.json\"\n",
    "\n",
    "    # Get credentials and create an API client\n",
    "    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n",
    "        client_secrets_file, scopes)\n",
    "\n",
    "    credentials = Credentials(\n",
    "        None,\n",
    "        refresh_token=\"1//0fNppFYz3o7ABCgYIARAAGA8SNwF-L9IrgIZJAKCn9iSH_172SxyT6cA3mMHDlSQ0MTj9MmKTc6zZRnSy1nwMW5kRkl52JYb4jhg\",\n",
    "        token_uri=\"https://accounts.google.com/o/oauth2/token\",\n",
    "        client_id=\"754636752811-rmth1g8e3dl144jda8fddh1ihhj413um.apps.googleusercontent.com\",\n",
    "        client_secret=\"KhUufHmhS8XI0srgpP__cTCr\")\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, credentials=credentials)\n",
    "\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        maxResults=topn,\n",
    "        q=keywords,\n",
    "        relevanceLanguage='en'\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items']\n",
    "\n",
    "\n",
    "\"\"\"     best_content_list = []\n",
    "    with os.scandir('./data') as entries:\n",
    "        for entry in entries:\n",
    "            with open('./data/' + entry.name, 'r') as file:\n",
    "                best_content_list.append(file.read())\n",
    "    return best_content_list \"\"\"\n",
    "\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Blob {} downloaded to {}.\".format(\n",
    "            source_blob_name, destination_file_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "def setupBiqqueryClient():\n",
    "    print('Loading ngram data ...')\n",
    "    global sql_conn\n",
    "    sql_conn = sqlite3.connect('ngram.db')\n",
    "\n",
    "    '''\n",
    "    global bigquery_data\n",
    "    bigquery_client = bigquery.Client()\n",
    "    table_id = 'ngram_reddit_2019_08.ngram_reddit_2019_08'\n",
    "    bigquery_data = {}\n",
    "    \n",
    "    NGRAM_FILE_NAME = 'ngram_2016_2019'\n",
    "    if not os.path.exists(NGRAM_FILE_NAME):\n",
    "        print('Downloading ngram from GCS ...')\n",
    "        download_blob('ventureum-ngram', NGRAM_FILE_NAME, NGRAM_FILE_NAME)\n",
    "        print('Download finished!')\n",
    "\n",
    "    with open(NGRAM_FILE_NAME, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data = json.loads(line)\n",
    "            bigquery_data[data['phrase']] = {'phrase': data['phrase'], 'volume_count': int(data['volume_count']), 'match_count': int(data['match_count'])}\n",
    "            '''\n",
    "\n",
    "def getKeyPhrases(content_list):\n",
    "    def insertFrequency(keyPhrase):\n",
    "        frequency = {'overall': 0, 'min': 0,\n",
    "                     'max': 0, 'frequency_by_content': []}\n",
    "        for content in content_list:\n",
    "            _freq = content.lower().count(keyPhrase['Text'].lower())\n",
    "            frequency['overall'] += _freq\n",
    "            frequency['min'] = _freq if frequency['min'] == 0 else min(\n",
    "                frequency['min'], _freq)\n",
    "            frequency['max'] = max(frequency['max'], _freq)\n",
    "            frequency['frequency_by_content'].append(_freq)\n",
    "\n",
    "\n",
    "        frequency['doc_freq'] = 0\n",
    "        for e in frequency['frequency_by_content']:\n",
    "            frequency['doc_freq'] += e > 0\n",
    "\n",
    "        frequency['median'] = statistics.median(\n",
    "            frequency['frequency_by_content'])\n",
    "\n",
    "        frequency['doc_freq_percent'] = float(frequency['doc_freq']) / len(frequency['frequency_by_content'])\n",
    "        return {**keyPhrase, 'frequency': frequency}\n",
    "\n",
    "\n",
    "    def getNgramDataFromPhraseFinder(phrase):\n",
    "        ngramData = {'text': phrase}\n",
    "\n",
    "        # Optional: set the maximum number of phrases to return.\n",
    "        options = pf.SearchOptions()\n",
    "        options.topk = 1\n",
    "\n",
    "        # Send the request.\n",
    "        try:\n",
    "            result = pf.search(pf.Corpus.AMERICAN_ENGLISH, phrase, options)\n",
    "            if result.error:\n",
    "                print('Request for {} was not successful: {}'.format(\n",
    "                    phrase, result.error['message']))\n",
    "                return\n",
    "\n",
    "            time.sleep(0.02)\n",
    "            ngramData = {\n",
    "                **ngramData, 'volume_count': result.phrases[0].volume_count,\n",
    "                'match_count': result.phrases[0].match_count}\n",
    "        except Exception as error:\n",
    "            print('Fatal error for {}: {}'.format(phrase, error))\n",
    "            ngramData = {**ngramData, 'volume_count': 0, 'match_count': 0}\n",
    "        \n",
    "        return ngramData\n",
    "\n",
    "    def getNgramDataFromBigQuery(phrase):\n",
    "        global bigquery_data\n",
    "\n",
    "        ngramData = {'text': phrase}\n",
    "        '''\n",
    "        if bigquery_data is None:\n",
    "            setupBiqqueryClient()\n",
    "        \n",
    "        client = bigquery.Client()\n",
    "        query_job = client.query(\n",
    "            \"\"\"\n",
    "            SELECT *\n",
    "            FROM `wallet-1546746037952.ngram_reddit_2019_08.ngram_reddit_2019_08`\n",
    "            WHERE phrase = \"{0}\"\n",
    "            \"\"\".format(phrase)\n",
    "        )\n",
    "        results = query_job.result()\n",
    "        \n",
    "        for row in results:\n",
    "            ngramData = {**ngramData, 'volume_count': row.volume_count, 'match_count': row.match_count}\n",
    "            return ngramData\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            if sql_conn is None:\n",
    "                setupBiqqueryClient()\n",
    "\n",
    "            _str = phrase.lower()\n",
    "            cur = sql_conn.cursor()\n",
    "            query = \"SELECT * FROM ngram_2016_2019 WHERE phrase = \\\"{}\\\"\".format(_str)\n",
    "            cur.execute(query)\n",
    "            row = cur.fetchone()\n",
    "            if row is None:\n",
    "                print('Cannot find {}'.format(_str))\n",
    "                ngramData = {**ngramData, 'volume_count': 0, 'match_count': 0}\n",
    "            else:\n",
    "                ngramData = {**ngramData, 'volume_count': row[2], 'match_count': row[1]}\n",
    "        except sqlite3.OperationalError as e:\n",
    "            print(e)\n",
    "            ngramData = {**ngramData, 'volume_count': 0, 'match_count': 0}\n",
    "        \n",
    "        return ngramData\n",
    "        \n",
    "\n",
    "    def insertNgramFrequency(keyPhrase):\n",
    "\n",
    "        db = TinyDB('db.json')\n",
    "        ngramQuery = Query()\n",
    "\n",
    "        # Set up your query.\n",
    "        query = keyPhrase['Text']\n",
    "\n",
    "        ngramData = db.get(ngramQuery.text == query)\n",
    "\n",
    "        if ngramData is None:\n",
    "            # ngramData = getNgramDataFromPhraseFinder(phrase)\n",
    "            ngramData = getNgramDataFromBigQuery(query)\n",
    "            # cache ngramData\n",
    "            \n",
    "            db.insert(ngramData)\n",
    "        return {**keyPhrase, 'ngram_volume_count': ngramData['volume_count'], 'ngram_match_count': ngramData['match_count']}\n",
    "\n",
    "    def filterPhrases(phrasesByContent):\n",
    "        phrases = reduce(lambda x, y: x+y, phrasesByContent)\n",
    "        # trim stop words from begining and end\n",
    "\n",
    "        def stripStopWords(phrase):\n",
    "            _stop_words = [\n",
    "                \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "                \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "                \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n",
    "                \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n",
    "                \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\",\n",
    "                \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\",\n",
    "                \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"in\", \"out\", \"on\", \"off\",\n",
    "                \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
    "                \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\",\n",
    "                \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\",\n",
    "                \"now\"] + [\"yeah\", \"thing\", \"things\", \"hey\", \"ohh\", \"really\", \"mine\", \"everybody\", \"anybody\", \"everyone\"]\n",
    "\n",
    "            while True:\n",
    "                # repetitively trim stop words from the beginning and the end\n",
    "                modified = False\n",
    "                _str = phrase['Text'].lower()\n",
    "                for w in _stop_words:\n",
    "                    if _str.startswith(w+' '):\n",
    "                        phrase['Text'] = phrase['Text'][len(w)+1:]\n",
    "                        modified = True\n",
    "                        break\n",
    "                    elif _str.endswith(' '+w):\n",
    "                        phrase['Text'] = phrase['Text'][:-len(w)-1]\n",
    "                        modified = True\n",
    "                        break\n",
    "                    elif _str == w:\n",
    "                        phrase['Text'] = \"\"\n",
    "                        break\n",
    "                if not modified:\n",
    "                    break\n",
    "            return phrase\n",
    "\n",
    "        \n",
    "        phrases = list(map(lambda x: {**x, 'ori_text': x['Text']}, phrases))\n",
    "\n",
    "        # substitue \\n with whitespace\n",
    "        phrases = list(map(lambda x: {**x, 'Text': x['Text'].replace(\"\\n\", \"\")}, phrases))\n",
    "\n",
    "        phrases = list(map(stripStopWords, phrases))\n",
    "\n",
    "        # only keep bigrams\n",
    "        phrases = list(filter(lambda x: len(\n",
    "            x['Text'].split(' ')) >= 1, phrases))\n",
    "\n",
    "        # remove duplicates\n",
    "        unique_phrases = []\n",
    "        phrase_set = set()\n",
    "        for phrase in phrases:\n",
    "            if phrase['Text'].lower() not in phrase_set:\n",
    "                phrase_set.add(phrase['Text'].lower())\n",
    "                unique_phrases.append(phrase)\n",
    "\n",
    "        print('# Unique phrases: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # min keyword score\n",
    "        unique_phrases = list(\n",
    "            filter(lambda x: x['Score'] >= 0.99, unique_phrases))\n",
    "\n",
    "        print('# phrases after min key word score: {}'.format(len(unique_phrases)))\n",
    "\n",
    "         # phrase should have at least 3 chars\n",
    "        unique_phrases = list(\n",
    "            filter(lambda x: len(x['Text']) >= 3, unique_phrases))\n",
    "        \n",
    "        print('# phrases after min chars: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # valid phrase should be words only, optionally connected by '-' or whitespace\n",
    "        unique_phrases = list(filter(lambda phrase: bool(re.fullmatch(r'^[a-zA-Z0-9-\\s]+$', phrase['Text'])), unique_phrases))\n",
    "\n",
    "        print('# phrases after words only: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # phrase should not contain only numbers, optionally connected by '-' or whitespace\n",
    "        unique_phrases = list(filter(lambda phrase: not bool(re.fullmatch(r'^[\\d\\s-]+$', phrase['Text'])), unique_phrases))\n",
    "\n",
    "        print('# phrases after not all numbers: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # add frequency for each phrase\n",
    "        unique_phrases = list(map(insertFrequency, unique_phrases))\n",
    "\n",
    "        # keep phrases showing up at least once in 10% of documents\n",
    "        unique_phrases = list(filter(lambda x: x['frequency']['doc_freq_percent'] >= 0.1, unique_phrases))\n",
    "\n",
    "        print('# phrases after doc_freq_percent: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # add ngrams frequency for each phrase\n",
    "        for i in tqdm(range(len(unique_phrases))):\n",
    "            unique_phrases[i] = insertNgramFrequency(unique_phrases[i])\n",
    "        #unique_phrases = list(map(insertNgramFrequency, unique_phrases))\n",
    "\n",
    "        phrases = unique_phrases\n",
    "\n",
    "        # sort key phrases by overall frequency\n",
    "        phrases.sort(reverse=True, key=lambda x: x['frequency']['overall'])\n",
    "\n",
    "        print(len(phrases))\n",
    "\n",
    "        # return most frequent top phrases\n",
    "        return phrases\n",
    "\n",
    "    # comprehend api requires each content to be less than 5000 bytes\n",
    "    # see https://docs.aws.amazon.com/comprehend/latest/dg/guidelines-and-limits.html\n",
    "    content_list_splited = []\n",
    "    for content in content_list:\n",
    "        content_list_splited.extend([content[i: i+4000]\n",
    "                                     for i in range(0, len(content), 4000)])\n",
    "    \"\"\" for content in content_list_splited:\n",
    "        print(len(content))\n",
    " \"\"\"\n",
    "    def chunks(l, n):\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "\n",
    "    content_list_splited_chunks = list(chunks(content_list_splited, 25))\n",
    "\n",
    "    keyPhrasesByContent = []\n",
    "    entitiesByContent = []\n",
    "    for chunk in content_list_splited_chunks:\n",
    "        keyPhrasesResponse = comprehend.batch_detect_key_phrases(\n",
    "            TextList=chunk, LanguageCode='en')\n",
    "        entitiesResponse = comprehend.batch_detect_entities(\n",
    "            TextList=chunk, LanguageCode='en')\n",
    "\n",
    "        keyPhrasesByContent.extend(\n",
    "            list(map(lambda x: x['KeyPhrases'], keyPhrasesResponse['ResultList'])))\n",
    "        entitiesByContent.extend(\n",
    "            list(map(lambda x: x['Entities'], entitiesResponse['ResultList'])))\n",
    "\n",
    "    # return filterPhrases(keyPhrasesByContent), filterPhrases(entitiesByContent)\n",
    "    return filterPhrases(keyPhrasesByContent)\n",
    "\n",
    "\n",
    "def getTranscript(video_id):\n",
    "    print('Getting transcript for '+video_id)\n",
    "    return YouTubeTranscriptApi.get_transcript(video_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords = 'how do i get my annual free credit report'\n",
    "keywords = 'How To Buy Your First Rental'\n",
    "print('Working on keywords: ' + keywords)\n",
    "video_list = getBestVideoList(keywords, 15)\n",
    "# video_list = [{'id': {'videoId': 'kqMtDrsc5Pw'}}]\n",
    "transcript_list = []\n",
    "for video in video_list:\n",
    "    try: \n",
    "        transcript_list.append(getTranscript(video['id']['videoId']))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "content_list = []\n",
    "for transcript in transcript_list:\n",
    "    content = '. '.join(list(map(lambda x: x['text'], transcript)))\n",
    "    # print(content)\n",
    "    # print(\"\\n\\n\")\n",
    "    content_list.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setupBiqqueryClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keyPhrases, entities = getKeyPhrases(content_list)\n",
    "keyPhrases = getKeyPhrases(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "total_words = 0\n",
    "def insertTfidf(phrase):\n",
    "    # 1.4 M books\n",
    "    # phrase['tfidf'] = (phrase['frequency']['overall'] / total_words) * (1+math.log(1180278/(1+phrase['ngram_volume_count'])))\n",
    "    phrase['tfidf'] = math.log((phrase['frequency']['overall']+1)) * (1+math.log(34863800/(1+phrase['ngram_volume_count'])))\n",
    "    # phrase['tfidf'] = (1) * (1+math.log(1180278/(1+phrase['ngram_volume_count'])))\n",
    "    return phrase\n",
    "\n",
    "def flatten(phrases):\n",
    "    rv = []\n",
    "    for phrase in phrases:\n",
    "        rv.append({\n",
    "            'phrase': phrase['Text'],\n",
    "            'ori_text': phrase['ori_text'],\n",
    "            'overall_freq': phrase['frequency']['overall'],\n",
    "            'min_freq': phrase['frequency']['min'],\n",
    "            'max_freq': phrase['frequency']['max'],\n",
    "            'median_freq': phrase['frequency']['median'],\n",
    "            'doc_freq_percent': phrase['frequency']['doc_freq_percent'],\n",
    "            'ngram_volume_count': phrase['ngram_volume_count'],\n",
    "            'ngram_match_count': phrase['ngram_match_count'],\n",
    "            'tfidf': phrase['tfidf']\n",
    "        })\n",
    "    return rv\n",
    "\n",
    "def printTable(phrases):\n",
    "    table = PrettyTable()\n",
    "    table.field_names=['phrase', 'ori_text', 'overall_freq', 'min_freq', 'max_freq', 'median_freq', 'doc_freq', 'ngram_volume_count', 'ngram_match_count', 'tfidf', 'entity_type']\n",
    "    for phrase in phrases:\n",
    "        entity_type = 'N/A'\n",
    "        if 'Type' in phrase:\n",
    "            entity_type = phrase['Type']\n",
    "        table.add_row([phrase['Text'], phrase['ori_text'], phrase['Score'], phrase['frequency']['overall'], phrase['frequency']['min'], phrase['frequency']['max'], phrase['frequency']['median'], phrase['frequency']['doc_freq'], phrase['ngram_volume_count'], phrase['ngram_match_count'], phrase['tfidf'], entity_type])\n",
    "    print(table)\n",
    "\n",
    "for content in content_list:\n",
    "    total_words += len(content.split())\n",
    "\n",
    "print(len(keyPhrases))\n",
    "keyPhrases = list(map(insertTfidf, keyPhrases))\n",
    "#print top keyphrases table\n",
    "keyPhrases.sort(reverse=True, key=lambda x: x['tfidf'])\n",
    "flattenKeyPhrases = flatten(keyPhrases)\n",
    "# printTable(keyPhrases)\n",
    "#printTable(entities)\n",
    "print(flattenKeyPhrases[5], flattenKeyPhrases[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def score(video_id, top_match=50):\n",
    "    # get transcript of the video\n",
    "    transcript = getTranscript(video_id)\n",
    "    transcript_content = '. '.join(list(map(lambda x: x['text'], transcript)))\n",
    "\n",
    "    report = {}\n",
    "    matched = 0\n",
    "    # check if keyword exist\n",
    "    for keyPhrase in keyPhrases[:top_match]:\n",
    "        _freq = transcript_content.lower().count(keyPhrase['Text'].lower())\n",
    "        report[keyPhrase['Text']] = _freq\n",
    "        matched += _freq > 0\n",
    "    \n",
    "    return matched*100.0/top_match, matched, report\n",
    "\n",
    "# ranking >= 16\n",
    "\n",
    "'''\n",
    "video_list = ['Uwl3-jBNEd4', '1zd2Ev1OUzI', 'C-0tIsUCa_A', 'Y4jYNlDZvPw', 'h8R0qQDQebM', 'yB5qMIYh09Y', 'LxAniFgeCOg', 'KaMwnQlVoMI', '22pEyDoTkUk', 'a5RExfeU4UY', 'CPJp9Ch37UM', 'eH5lvuxvdCk', 'pjuRMKnVqhY', 'OXWssHtrteo', 'hdLPGmYHstg', '1i1KYJBPOzo', 'aGl3glZN74o', 'rFN_VOQjVqE', 'sM_DcT9ejdY', 'ghpYdSiLIyo', 'Ogre88cedwM', 'qs8TzflEkQg', '_HX22c0YD2s', 'UALlQsrbFas']\n",
    "'''\n",
    "\n",
    "# > 5m, 2k<=views<3k\n",
    "'''\n",
    "video_list = ['Zt7veuZb4_M', 'C-0tIsUCa_A', 'vMZSFGpAtcM', 'AY5jGiPiIBw', 'uiW8DHXpbNg', 'awLunjakH3Y', 'OBUl9ucfvGI', 'n6EFBt7BUPY', 'pCMsvfvnTXc', 'te9fn916zBY']\n",
    "'''\n",
    "\n",
    "# > 5m, 1k<=views<2k\n",
    "'''\n",
    "video_list = ['R_PB62qLrHE', '6sElkS_VjrI', 'EJ3haVDjgXY', '6X-SOg0-DDo', 'CZNvFLlDjgY', 'x7JosBVI9es', '4uXfwVt1wQU', 'tNuidxXRuDY', '2LucrGdfgFc', 'RXx73tPaoe8']\n",
    "'''\n",
    "\n",
    "# > 5m, 500<=views<1k\n",
    "'''\n",
    "video_list = ['UALlQsrbFas', 'zfkm53ehcQ0', 'IjPBzpkPFao', 'SR4FTQt9jYY', 'S_eMgsv5G1k', 'Z6sNAOCKEbM', 'SMqvz0rp22w', 'ZUDfRs3oLic', 'RcKxk6se4Us', 'LxL6bBTkGSo']\n",
    "'''\n",
    "\n",
    "# > 5m, 370<views<500\n",
    "video_list = ['O76_DSyWbiU', 'KCLs7NmveIs', 'Euj3FyBisfA', 'P7_X25yr75w', 'ze3GLDE-c7U', 'BJXP4gt3gik', 'VwmGaiKkbiE', 'ElMgmKqtnTc', 'mjT7Lxe-lvU', '_GA-TMrG8RA']\n",
    "\n",
    "print(\"testing video count: {}\".format(len(video_list)))\n",
    "chart_data = []\n",
    "for video_id in video_list:\n",
    "    try:\n",
    "        match_perc, matched, report = score(video_id)\n",
    "        print(match_perc, matched)\n",
    "        chart_data.append(match_perc)\n",
    "    except Exception as e:\n",
    "        print('error scoring for video {} with err {}'.format(video_id, e))\n",
    "\n",
    "print('median {}'.format(statistics.median(chart_data)))\n",
    "print('mean {}'.format(statistics.mean(chart_data)))\n",
    "plt.plot(chart_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}