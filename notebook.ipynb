{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from functools import reduce\n",
    "import pprint\n",
    "from prettytable import PrettyTable\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "from phrasefinder import phrasefinder as pf\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "from functools import reduce\n",
    "from tinydb import TinyDB, Query\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient\n",
    "from google.cloud.bigquery_storage import types\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import re\n",
    "import os.path\n",
    "comprehend = boto3.client(service_name='comprehend', region_name='us-east-1')\n",
    "\n",
    "\n",
    "bigquery_client = None\n",
    "bigquery_data = None\n",
    "\n",
    "def getBestVideoList(keywords, topn=10):\n",
    "    scopes = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"0\"\n",
    "\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    client_secrets_file = \"./google_credentials/client_secret_754636752811-rmth1g8e3dl144jda8fddh1ihhj413um.apps.googleusercontent.com.json\"\n",
    "\n",
    "    # Get credentials and create an API client\n",
    "    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n",
    "        client_secrets_file, scopes)\n",
    "\n",
    "    credentials = Credentials(\n",
    "        None,\n",
    "        refresh_token=\"1//0fNppFYz3o7ABCgYIARAAGA8SNwF-L9IrgIZJAKCn9iSH_172SxyT6cA3mMHDlSQ0MTj9MmKTc6zZRnSy1nwMW5kRkl52JYb4jhg\",\n",
    "        token_uri=\"https://accounts.google.com/o/oauth2/token\",\n",
    "        client_id=\"754636752811-rmth1g8e3dl144jda8fddh1ihhj413um.apps.googleusercontent.com\",\n",
    "        client_secret=\"KhUufHmhS8XI0srgpP__cTCr\")\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, credentials=credentials)\n",
    "\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        maxResults=topn,\n",
    "        q=keywords,\n",
    "        relevanceLanguage='en'\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items']\n",
    "\n",
    "\n",
    "\"\"\"     best_content_list = []\n",
    "    with os.scandir('./data') as entries:\n",
    "        for entry in entries:\n",
    "            with open('./data/' + entry.name, 'r') as file:\n",
    "                best_content_list.append(file.read())\n",
    "    return best_content_list \"\"\"\n",
    "\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Blob {} downloaded to {}.\".format(\n",
    "            source_blob_name, destination_file_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "def setupBiqqueryClient():\n",
    "    print('Loading ngram data ...')\n",
    "    global bigquery_data\n",
    "    bigquery_client = bigquery.Client()\n",
    "    table_id = 'ngram_reddit_2019_08.ngram_reddit_2019_08'\n",
    "    bigquery_data = {}\n",
    "    \n",
    "    NGRAM_FILE_NAME = 'ngram_2016_2019_full'\n",
    "    if not path.exists(NGRAM_FILE_NAME):\n",
    "        print('Downloading ngram from GCS ...')\n",
    "        download_blob('ventureum-ngram', NGRAM_FILE_NAME, NGRAM_FILE_NAME)\n",
    "        print('Download finished!')\n",
    "\n",
    "    with open(NGRAM_FILE_NAME, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data = json.loads(line)\n",
    "            bigquery_data[data['phrase']] = {'phrase': data['phrase'], 'volume_count': int(data['volume_count']), 'match_count': int(data['match_count'])}\n",
    "\n",
    "def getKeyPhrases(content_list):\n",
    "    def insertFrequency(keyPhrase):\n",
    "        frequency = {'overall': 0, 'min': 0,\n",
    "                     'max': 0, 'frequency_by_content': []}\n",
    "        for content in content_list:\n",
    "            _freq = content.count(keyPhrase['Text'])\n",
    "            frequency['overall'] += _freq\n",
    "            frequency['min'] = _freq if frequency['min'] == 0 else min(\n",
    "                frequency['min'], _freq)\n",
    "            frequency['max'] = max(frequency['max'], _freq)\n",
    "            frequency['frequency_by_content'].append(_freq)\n",
    "\n",
    "\n",
    "        frequency['doc_freq'] = 0\n",
    "        for e in frequency['frequency_by_content']:\n",
    "            frequency['doc_freq'] += e > 0\n",
    "\n",
    "        frequency['median'] = statistics.median(\n",
    "            frequency['frequency_by_content'])\n",
    "\n",
    "        frequency['doc_freq_percent'] = float(frequency['doc_freq']) / len(frequency['frequency_by_content'])\n",
    "        return {**keyPhrase, 'frequency': frequency}\n",
    "\n",
    "\n",
    "    def getNgramDataFromPhraseFinder(phrase):\n",
    "        ngramData = {'text': phrase}\n",
    "\n",
    "        # Optional: set the maximum number of phrases to return.\n",
    "        options = pf.SearchOptions()\n",
    "        options.topk = 1\n",
    "\n",
    "        # Send the request.\n",
    "        try:\n",
    "            result = pf.search(pf.Corpus.AMERICAN_ENGLISH, phrase, options)\n",
    "            if result.error:\n",
    "                print('Request for {} was not successful: {}'.format(\n",
    "                    phrase, result.error['message']))\n",
    "                return\n",
    "\n",
    "            time.sleep(0.02)\n",
    "            ngramData = {\n",
    "                **ngramData, 'volume_count': result.phrases[0].volume_count,\n",
    "                'match_count': result.phrases[0].match_count}\n",
    "        except Exception as error:\n",
    "            print('Fatal error for {}: {}'.format(phrase, error))\n",
    "            ngramData = {**ngramData, 'volume_count': 0, 'match_count': 0}\n",
    "        \n",
    "        return ngramData\n",
    "\n",
    "    def getNgramDataFromBigQuery(phrase):\n",
    "        global bigquery_data\n",
    "\n",
    "        ngramData = {'text': phrase}\n",
    "\n",
    "        if bigquery_data is None:\n",
    "            setupBiqqueryClient()\n",
    "        '''\n",
    "        client = bigquery.Client()\n",
    "        query_job = client.query(\n",
    "            \"\"\"\n",
    "            SELECT *\n",
    "            FROM `wallet-1546746037952.ngram_reddit_2019_08.ngram_reddit_2019_08`\n",
    "            WHERE phrase = \"{0}\"\n",
    "            \"\"\".format(phrase)\n",
    "        )\n",
    "        results = query_job.result()\n",
    "        \n",
    "        for row in results:\n",
    "            ngramData = {**ngramData, 'volume_count': row.volume_count, 'match_count': row.match_count}\n",
    "            return ngramData\n",
    "        '''\n",
    "\n",
    "        _str = phrase.lower()\n",
    "        if _str not in bigquery_data:\n",
    "            print('Fatal error for {}'.format(_str))\n",
    "            ngramData = {**ngramData, 'volume_count': 0, 'match_count': 0}\n",
    "        else:\n",
    "            ngramData = bigquery_data[_str]\n",
    "        return ngramData\n",
    "        \n",
    "\n",
    "    def insertNgramFrequency(keyPhrase):\n",
    "\n",
    "        db = TinyDB('db.json')\n",
    "        ngramQuery = Query()\n",
    "\n",
    "        # Set up your query.\n",
    "        query = keyPhrase['Text']\n",
    "\n",
    "        ngramData = db.get(ngramQuery.text == query)\n",
    "\n",
    "        if ngramData is None:\n",
    "            # ngramData = getNgramDataFromPhraseFinder(phrase)\n",
    "            ngramData = getNgramDataFromBigQuery(query)\n",
    "            # cache ngramData\n",
    "            db.insert(ngramData)\n",
    "\n",
    "        return {**keyPhrase, 'ngram_volume_count': ngramData['volume_count'], 'ngram_match_count': ngramData['match_count']}\n",
    "\n",
    "    def filterPhrases(phrasesByContent):\n",
    "        phrases = reduce(lambda x, y: x+y, phrasesByContent)\n",
    "        # trim stop words from begining and end\n",
    "\n",
    "        def stripStopWords(phrase):\n",
    "            _stop_words = [\n",
    "                \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "                \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "                \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n",
    "                \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n",
    "                \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\",\n",
    "                \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\",\n",
    "                \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"in\", \"out\", \"on\", \"off\",\n",
    "                \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
    "                \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\",\n",
    "                \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\",\n",
    "                \"now\"] + [\"yeah\", \"thing\", \"things\", \"hey\", \"ohh\", \"really\", \"mine\", \"everybody\", \"anybody\", \"everyone\"]\n",
    "\n",
    "            while True:\n",
    "                # repetitively trim stop words from the beginning and the end\n",
    "                modified = False\n",
    "                _str = phrase['Text'].lower()\n",
    "                for w in _stop_words:\n",
    "                    if _str.startswith(w+' '):\n",
    "                        phrase['Text'] = phrase['Text'][len(w)+1:]\n",
    "                        modified = True\n",
    "                        break\n",
    "                    elif _str.endswith(' '+w):\n",
    "                        phrase['Text'] = phrase['Text'][:-len(w)-1]\n",
    "                        modified = True\n",
    "                        break\n",
    "                    elif _str == w:\n",
    "                        phrase['Text'] = \"\"\n",
    "                        break\n",
    "                if not modified:\n",
    "                    break\n",
    "            return phrase\n",
    "\n",
    "        phrases = list(map(lambda x: {**x, 'ori_text': x['Text']}, phrases))\n",
    "\n",
    "        phrases = list(map(stripStopWords, phrases))\n",
    "\n",
    "        # only keep bigrams\n",
    "        phrases = list(filter(lambda x: len(\n",
    "            x['Text'].split(' ')) >= 1, phrases))\n",
    "\n",
    "        # remove duplicates\n",
    "        unique_phrases = []\n",
    "        phrase_set = set()\n",
    "        for phrase in phrases:\n",
    "            if phrase['Text'] not in phrase_set:\n",
    "                phrase_set.add(phrase['Text'])\n",
    "                unique_phrases.append(phrase)\n",
    "\n",
    "        print('# Unique phrases: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # min keyword score\n",
    "        unique_phrases = list(\n",
    "            filter(lambda x: x['Score'] >= 0.99, unique_phrases))\n",
    "\n",
    "        print('# phrases after min key word score: {}'.format(len(unique_phrases)))\n",
    "\n",
    "         # phrase should have at least 3 chars\n",
    "        unique_phrases = list(\n",
    "            filter(lambda x: len(x['Text']) >= 3, unique_phrases))\n",
    "        \n",
    "        print('# phrases after min chars: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # valid phrase should be words only, optionally connected by '-' or whitespace\n",
    "        unique_phrases = list(filter(lambda phrase: bool(re.fullmatch(r'^[a-zA-Z0-9-\\s]+$', phrase['Text'])), unique_phrases))\n",
    "\n",
    "        print('# phrases after words only: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # phrase should not contain only numbers, optionally connected by '-' or whitespace\n",
    "        unique_phrases = list(filter(lambda phrase: not bool(re.fullmatch(r'^[\\d\\s-]+$', phrase['Text'])), unique_phrases))\n",
    "\n",
    "        print('# phrases after not all numbers: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # add frequency for each phrase\n",
    "        unique_phrases = list(map(insertFrequency, unique_phrases))\n",
    "\n",
    "        # keep phrases showing up at least once in 10% of documents\n",
    "        unique_phrases = list(filter(lambda x: x['frequency']['doc_freq_percent'] >= 0.1, unique_phrases))\n",
    "\n",
    "        print('# phrases after doc_freq_percent: {}'.format(len(unique_phrases)))\n",
    "\n",
    "        # add ngrams frequency for each phrase\n",
    "        for i in tqdm(range(len(unique_phrases))):\n",
    "            unique_phrases[i] = insertNgramFrequency(unique_phrases[i])\n",
    "        #unique_phrases = list(map(insertNgramFrequency, unique_phrases))\n",
    "\n",
    "        phrases = unique_phrases\n",
    "\n",
    "        # sort key phrases by overall frequency\n",
    "        phrases.sort(reverse=True, key=lambda x: x['frequency']['overall'])\n",
    "\n",
    "        print(len(phrases))\n",
    "\n",
    "        # return most frequent top phrases\n",
    "        return phrases\n",
    "\n",
    "    # comprehend api requires each content to be less than 5000 bytes\n",
    "    # see https://docs.aws.amazon.com/comprehend/latest/dg/guidelines-and-limits.html\n",
    "    content_list_splited = []\n",
    "    for content in content_list:\n",
    "        content_list_splited.extend([content[i: i+4000]\n",
    "                                     for i in range(0, len(content), 4000)])\n",
    "    \"\"\" for content in content_list_splited:\n",
    "        print(len(content))\n",
    " \"\"\"\n",
    "    def chunks(l, n):\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "\n",
    "    content_list_splited_chunks = list(chunks(content_list_splited, 25))\n",
    "\n",
    "    keyPhrasesByContent = []\n",
    "    entitiesByContent = []\n",
    "    for chunk in content_list_splited_chunks:\n",
    "        keyPhrasesResponse = comprehend.batch_detect_key_phrases(\n",
    "            TextList=chunk, LanguageCode='en')\n",
    "        entitiesResponse = comprehend.batch_detect_entities(\n",
    "            TextList=chunk, LanguageCode='en')\n",
    "\n",
    "        keyPhrasesByContent.extend(\n",
    "            list(map(lambda x: x['KeyPhrases'], keyPhrasesResponse['ResultList'])))\n",
    "        entitiesByContent.extend(\n",
    "            list(map(lambda x: x['Entities'], entitiesResponse['ResultList'])))\n",
    "\n",
    "    # return filterPhrases(keyPhrasesByContent), filterPhrases(entitiesByContent)\n",
    "    return filterPhrases(keyPhrasesByContent)\n",
    "\n",
    "\n",
    "def getTranscript(video_id):\n",
    "    print('Getting transcript for '+video_id)\n",
    "    return YouTubeTranscriptApi.get_transcript(video_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Working on keywords: How To Buy Your First Rental\n",
      "Getting transcript for bJx7_1rWC6U\n",
      "Getting transcript for 7TB_eRhSNV4\n",
      "Getting transcript for u83O2l1QEj4\n",
      "Getting transcript for LxAniFgeCOg\n",
      "Getting transcript for IOqlt3dwb_c\n",
      "Getting transcript for zoCGqNkBsbA\n",
      "Getting transcript for qiKg3790dtA\n",
      "Getting transcript for nb9gtT-BqRc\n",
      "Getting transcript for a5RExfeU4UY\n",
      "Getting transcript for yv4ZIaiRMZM\n",
      "Getting transcript for zV7ESWTdKQ0\n",
      "Getting transcript for CPJp9Ch37UM\n",
      "Getting transcript for ShOrTMtsxuc\n",
      "Getting transcript for vMZSFGpAtcM\n",
      "Getting transcript for _HX22c0YD2s\n"
     ]
    }
   ],
   "source": [
    "# keywords = 'how do i get my annual free credit report'\n",
    "keywords = 'How To Buy Your First Rental'\n",
    "print('Working on keywords: ' + keywords)\n",
    "video_list = getBestVideoList(keywords, 15)\n",
    "# video_list = [{'id': {'videoId': 'kqMtDrsc5Pw'}}]\n",
    "transcript_list = []\n",
    "for video in video_list:\n",
    "    try: \n",
    "        transcript_list.append(getTranscript(video['id']['videoId']))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "content_list = []\n",
    "for transcript in transcript_list:\n",
    "    content = '. '.join(list(map(lambda x: x['text'], transcript)))\n",
    "    # print(content)\n",
    "    # print(\"\\n\\n\")\n",
    "    content_list.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading ngram data ...\n"
     ]
    }
   ],
   "source": [
    "setupBiqqueryClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# Unique phrases: 3377\n",
      "# phrases after min key word score: 1895\n",
      "# phrases after min chars: 1873\n",
      "# phrases after words only: 1811\n",
      "# phrases after not all numbers: 1790\n",
      "  0%|          | 1/637 [00:00<01:17,  8.18it/s]# phrases after doc_freq_percent: 637\n",
      "Fatal error for first rental property\n",
      "  4%|▍         | 26/637 [00:03<01:05,  9.31it/s]Fatal error for 740 credit\n",
      " 24%|██▍       | 156/637 [00:20<01:08,  7.05it/s]Fatal error for step number seven\n",
      " 29%|██▉       | 187/637 [00:24<00:59,  7.53it/s]Fatal error for ten units\n",
      " 39%|███▉      | 251/637 [00:32<00:47,  8.18it/s]Fatal error for local lender\n",
      " 42%|████▏     | 270/637 [00:35<00:45,  8.12it/s]Fatal error for monthly property tax\n",
      " 47%|████▋     | 301/637 [00:38<00:49,  6.79it/s]Fatal error for house hack\n",
      " 49%|████▊     | 309/637 [00:40<00:47,  6.86it/s]Fatal error for single deal\n",
      " 51%|█████▏    | 328/637 [00:42<00:41,  7.53it/s]Fatal error for escrow process\n",
      " 54%|█████▍    | 343/637 [00:44<00:39,  7.47it/s]Fatal error for estate investment property\n",
      " 57%|█████▋    | 360/637 [00:47<00:36,  7.66it/s]Fatal error for step number one\n",
      " 59%|█████▉    | 376/637 [00:49<00:41,  6.21it/s]Fatal error for investment loan\n",
      " 59%|█████▉    | 378/637 [00:49<00:36,  7.04it/s]Fatal error for different lenders\n",
      " 61%|██████    | 386/637 [00:50<00:33,  7.42it/s]Fatal error for beardy brandon\n",
      " 73%|███████▎  | 467/637 [01:01<00:23,  7.31it/s]Fatal error for four plex\n",
      " 79%|███████▉  | 502/637 [01:06<00:17,  7.54it/s]Fatal error for property problems\n",
      " 86%|████████▋ | 551/637 [01:13<00:12,  6.68it/s]Fatal error for next property\n",
      " 88%|████████▊ | 563/637 [01:14<00:09,  8.16it/s]Fatal error for long-term wealth\n",
      " 94%|█████████▎| 597/637 [01:19<00:05,  7.08it/s]Fatal error for full-time income\n",
      " 97%|█████████▋| 618/637 [01:22<00:02,  7.15it/s]Fatal error for property problem\n",
      "100%|██████████| 637/637 [01:24<00:00,  7.52it/s]637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# keyPhrases, entities = getKeyPhrases(content_list)\n",
    "keyPhrases = getKeyPhrases(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "637\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "total_words = 0\n",
    "def insertTfidf(phrase):\n",
    "    # 1.4 M books\n",
    "    # phrase['tfidf'] = (phrase['frequency']['overall'] / total_words) * (1+math.log(1180278/(1+phrase['ngram_volume_count'])))\n",
    "    phrase['tfidf'] = math.log((phrase['frequency']['overall']+1)) * (1+math.log(1180278/(1+phrase['ngram_volume_count'])))\n",
    "    # phrase['tfidf'] = (1) * (1+math.log(1180278/(1+phrase['ngram_volume_count'])))\n",
    "    return phrase\n",
    "\n",
    "def flatten(phrases):\n",
    "    rv = []\n",
    "    for phrase in phrases:\n",
    "        rv.append({\n",
    "            'phrase': phrase['Text'],\n",
    "            'ori_text': phrase['ori_text'],\n",
    "            'overall_freq': phrase['frequency']['overall'],\n",
    "            'min_freq': phrase['frequency']['min'],\n",
    "            'max_freq': phrase['frequency']['max'],\n",
    "            'median_freq': phrase['frequency']['median'],\n",
    "            'doc_freq_percent': phrase['frequency']['doc_freq_percent'],\n",
    "            'ngram_volume_count': phrase['ngram_volume_count'],\n",
    "            'ngram_match_count': phrase['ngram_match_count'],\n",
    "            'tfidf': phrase['tfidf']\n",
    "        })\n",
    "    return rv\n",
    "\n",
    "def printTable(phrases):\n",
    "    table = PrettyTable()\n",
    "    table.field_names=['phrase', 'ori_text', 'overall_freq', 'min_freq', 'max_freq', 'median_freq', 'doc_freq', 'ngram_volume_count', 'ngram_match_count', 'tfidf', 'entity_type']\n",
    "    for phrase in phrases:\n",
    "        entity_type = 'N/A'\n",
    "        if 'Type' in phrase:\n",
    "            entity_type = phrase['Type']\n",
    "        table.add_row([phrase['Text'], phrase['ori_text'], phrase['Score'], phrase['frequency']['overall'], phrase['frequency']['min'], phrase['frequency']['max'], phrase['frequency']['median'], phrase['frequency']['doc_freq'], phrase['ngram_volume_count'], phrase['ngram_match_count'], phrase['tfidf'], entity_type])\n",
    "    print(table)\n",
    "\n",
    "for content in content_list:\n",
    "    total_words += len(content.split())\n",
    "\n",
    "print(len(keyPhrases))\n",
    "keyPhrases = list(map(insertTfidf, keyPhrases))\n",
    "#print top keyphrases table\n",
    "keyPhrases.sort(reverse=True, key=lambda x: x['tfidf'])\n",
    "flattenKeyPhrases = flatten(keyPhrases)\n",
    "# printTable(keyPhrases)\n",
    "#printTable(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}